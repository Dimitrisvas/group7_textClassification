{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0746eec4f0cf2bd2bd143a4eb10480580d722f553ed0fc03fc42076491106f879",
   "display_name": "Python 3.8.8 64-bit ('grp7_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pickle\n",
    "import keras\n",
    "import talos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Dropout, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Constants\n",
    "EPOCHS = 30\n",
    "INIT_LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in pre-processed dataset\n",
    "x_text_train = pickle.load(open('../comment_lemma.pickle', 'rb'))\n",
    "y_text_train = pd.read_pickle('../balanced_dataset.pickle').drop(columns='comment_text')\n",
    "x_text_test = pickle.load(open('../balanced_test_dataset.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=100000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(x_text_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(x_text_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(x_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpadlen = 200\n",
    "X_t=pad_sequences(list_tokenized_train, maxlen=maxpadlen, padding = 'post')\n",
    "X_te=pad_sequences(list_tokenized_test, maxlen=maxpadlen, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 0.2\n",
    "num_validation_samples = int(val_split*X_t.shape[0])\n",
    "x_train = X_t[: -num_validation_samples]\n",
    "y_train = y_text_train[: -num_validation_samples]\n",
    "x_val = X_t[-num_validation_samples: ]\n",
    "y_val = y_text_train[-num_validation_samples: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim_fasttext = 300\n",
    "embeddings_index_fasttext = {}\n",
    "f = open('wiki-news-300d-1M.vec', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    embeddings_index_fasttext[word] = np.asarray(values[1:], dtype='float32')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_fasttext = np.random.random((len(tokenizer.word_index) + 1, embedding_dim_fasttext))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index_fasttext.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_fasttext[i] = embedding_vector\n",
    "print(\" Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Input Parameters to the Function.\n",
    "def toxic_classifier(x_train,y_train,x_val,y_val,params):\n",
    "\n",
    "  inp=Input(shape=(maxpadlen, ),dtype='int32')\n",
    "\n",
    "  embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                           embedding_dim_fasttext,\n",
    "                           weights = [embedding_matrix_fasttext],\n",
    "                           input_length = maxpadlen,\n",
    "                           trainable=False,\n",
    "                           name = 'embeddings')\n",
    "  embedded_sequences = embedding_layer(inp)\n",
    "\n",
    "  x = LSTM(params['output_count_lstm'], return_sequences=True,name='lstm_layer')(embedded_sequences)\n",
    "  \n",
    "  x = GlobalMaxPool1D()(x)\n",
    "  \n",
    "  x = Dropout(params['dropout'])(x)\n",
    "  \n",
    "  x = Dense(params['output_count_dense'], activation=params['activation'], kernel_initializer='he_uniform')(x)\n",
    "  \n",
    "  x = Dropout(params['dropout'])(x)\n",
    "  \n",
    "  preds = Dense(6, activation=params['last_activation'], kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "  model = Model(inputs=inp, outputs=preds)\n",
    "\n",
    "  model.compile(loss=params['loss'], optimizer=params['optimizer'], metrics=['accuracy'])\n",
    "\n",
    "  model_info=model.fit(x_train,y_train, epochs=params['epochs'], batch_size=params['batch_size'],  validation_data=(x_val, y_val))\n",
    "\n",
    "  return model_info, model\n",
    "\n",
    "#Creating a dictionary of Parameters.\n",
    "p={\n",
    "    'output_count_lstm': [40,50,60],\n",
    "    'output_count_dense': [30,40,50],\n",
    "    'batch_size': [32],\n",
    "    'epochs':[2],\n",
    "    'optimizer':['adam'],\n",
    "    'activation':['relu'],\n",
    "    'last_activation': ['sigmoid'],\n",
    "    'dropout':[0.1,0.2],\n",
    "    'loss': ['binary_crossentropy']   \n",
    "}\n",
    "\n",
    "#Initiating GridSearchCV.\n",
    "scan_results = talos.Scan(x=x_train,\n",
    "               y=y_train,\n",
    "               x_val=x_val,\n",
    "               y_val=y_val,\n",
    "               model=toxic_classifier,\n",
    "               params=p,\n",
    "               experiment_name='tcc',\n",
    "               print_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM Model.\r\n",
    "inp=Input(shape=(maxpadlen, ),dtype='int32')\r\n",
    "embedding_layer = Embedding(len(tokenizer.word_index) + 1,\r\n",
    "                           embedding_dim_fasttext,\r\n",
    "                           weights = [embedding_matrix_fasttext],\r\n",
    "                           input_length = maxpadlen,\r\n",
    "                           trainable=False,\r\n",
    "                           name = 'embeddings')\r\n",
    "embedded_sequences = embedding_layer(inp)\r\n",
    "x = LSTM(50, return_sequences=True,name='lstm_layer')(embedded_sequences)\r\n",
    "x = GlobalMaxPool1D()(x)\r\n",
    "x = Dropout(0.2)(x)\r\n",
    "x = Dense(40, activation=\"relu\", kernel_initializer='he_uniform')(x)\r\n",
    "x = Dropout(0.2)(x)\r\n",
    "preds = Dense(6, activation=\"sigmoid\", kernel_initializer='glorot_uniform')(x)\r\n",
    "\r\n",
    "#Compile the Model.\r\n",
    "model_1 = Model(inputs=inp, outputs=preds)\r\n",
    "model_1.compile(loss='binary_crossentropy',\r\n",
    "                  optimizer='adam',\r\n",
    "                  metrics=['accuracy'])\r\n",
    "\r\n",
    "#Fit the Model on Training Data.\r\n",
    "model_info_1=model_1.fit(x_train,y_train, epochs=2, batch_size=32,  validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Input Parameters to the Function.\n",
    "def toxic_classifier(x_train,y_train,x_val,y_val,params):\n",
    "\n",
    "  inp=Input(shape=(maxpadlen, ),dtype='int32')\n",
    "\n",
    "  embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                           embedding_dim_fasttext,\n",
    "                           weights = [embedding_matrix_fasttext],\n",
    "                           input_length = maxpadlen,\n",
    "                           trainable=False,\n",
    "                           name = 'embeddings')\n",
    "  embedded_sequences = embedding_layer(inp)\n",
    "\n",
    "  x = LSTM(params['output_count_lstm'], return_sequences=True,name='lstm_layer')(embedded_sequences)\n",
    "\n",
    "  x = Conv1D(filters=params['filters'], kernel_size=params['kernel_size'], padding='same', activation='relu', kernel_initializer='he_uniform')(x)\n",
    "\n",
    "  x = MaxPooling1D(params['pool_size'])(x)\n",
    "  \n",
    "  x = GlobalMaxPool1D()(x)\n",
    "  \n",
    "  x = BatchNormalization()(x)\n",
    "  \n",
    "  x = Dense(params['output_1_count_dense'], activation=params['activation'], kernel_initializer='he_uniform')(x)\n",
    "  \n",
    "  x = Dropout(params['dropout'])(x)\n",
    "\n",
    "  x = Dense(params['output_2_count_dense'], activation=params['activation'], kernel_initializer='he_uniform')(x)\n",
    "  \n",
    "  x = Dropout(params['dropout'])(x)\n",
    "  \n",
    "  preds = Dense(6, activation=params['last_activation'], kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "  model = Model(inputs=inp, outputs=preds)\n",
    "\n",
    "  model.compile(loss=params['loss'], optimizer=params['optimizer'], metrics=['accuracy'])\n",
    "\n",
    "  model_info=model.fit(x_train,y_train, epochs=params['epochs'], batch_size=params['batch_size'],  validation_data=(x_val, y_val))\n",
    "\n",
    "  return model_info, model\n",
    "\n",
    "#Creating a dictionary of Parameters.\n",
    "p={\n",
    "    'output_count_lstm': [50,60],\n",
    "    'output_1_count_dense': [40,50],\n",
    "    'output_2_count_dense': [30,40],\n",
    "    'filters' : [64],\n",
    "    'kernel_size' : [3],\n",
    "    'batch_size': [32],\n",
    "    'pool_size': [3],\n",
    "    'epochs':[2],\n",
    "    'optimizer':['adam'],\n",
    "    'activation':['relu'],\n",
    "    'last_activation': ['sigmoid'],\n",
    "    'dropout':[0.1,0.2],\n",
    "    'loss': ['binary_crossentropy']   \n",
    "}\n",
    "\n",
    "#Initiating GridSearchCV.\n",
    "scan_results = talos.Scan(x=x_train,\n",
    "               y=y_train,\n",
    "               x_val=x_val,\n",
    "               y_val=y_val,\n",
    "               model=toxic_classifier,\n",
    "               params=p,\n",
    "               experiment_name='tcc',\n",
    "               print_params=True)\n",
    "\n",
    "# Define the LSTM-CNN Model.\n",
    "inp=Input(shape=(maxpadlen, ),dtype='int32')\n",
    "embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                           embedding_dim_fasttext,\n",
    "                           weights = [embedding_matrix_fasttext],\n",
    "                           input_length = maxpadlen,\n",
    "                           trainable=False,\n",
    "                           name = 'embeddings')\n",
    "embedded_sequences = embedding_layer(inp)\n",
    "x = LSTM(50, return_sequences=True,name='lstm_layer')(embedded_sequences)\n",
    "x = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_uniform')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(40, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(30, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "preds = Dense(6, activation=\"sigmoid\", kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "#Compile the Model.\n",
    "model_2 = Model(inputs=inp, outputs=preds)\n",
    "model_2.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "#Fit the Model on Training Data.\n",
    "model_info_2=model_2.fit(x_train,y_train, epochs=2, batch_size=32,  validation_data=(x_val, y_val))"
   ]
  }
 ]
}