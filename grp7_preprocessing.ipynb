{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wrapped-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "# Pre-processing imports\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "golden-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TRAIN_DATASET = \"train.csv\"\n",
    "TEST_DATA = \"test.csv\"\n",
    "TEST_LABELS = \"test_labels.csv\"\n",
    "DATA_FIELD = [\"comment_text\"]\n",
    "LABEL_FIELDS = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "REDUNDANT_FIELDS = [\"id\"]\n",
    "STOP_WORDS = set(stopwords.words('english')) # Stopwords in English only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "straight-consistency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment_text     7132\n",
      "toxic            7132\n",
      "severe_toxic     7132\n",
      "obscene          7132\n",
      "threat           7132\n",
      "insult           7132\n",
      "identity_hate    7132\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read in training dataset\n",
    "train_dataset = pd.read_csv(TRAIN_DATASET)\n",
    "\n",
    "# # Split training_data into x_train and y_train -- SAVE FOR LATER\n",
    "# x_train = training_data[DATA_FIELD]\n",
    "# y_train = training_data[LABEL_FIELDS]\n",
    "\n",
    "# Read in test data\n",
    "test_data = pd.read_csv(TEST_DATA)\n",
    "test_labels = pd.read_csv(TEST_LABELS)\n",
    "\n",
    "# Combine test data and labels into one data frame\n",
    "test_dataset = pd.concat([test_data, test_labels], axis=1)\n",
    "\n",
    "# Remove redundant id field from both datasets\n",
    "train_dataset = train_dataset.drop(columns=REDUNDANT_FIELDS)\n",
    "test_dataset = test_dataset.drop(columns=REDUNDANT_FIELDS)\n",
    "\n",
    "# Remove samples with labels containing -1 in test dataset, this \n",
    "# is a place holder for samples that were not assigned labels.\n",
    "test_dataset = test_dataset.drop(test_dataset[(test_dataset.toxic == -1) |\n",
    "                                              (test_dataset.severe_toxic == -1) |\n",
    "                                              (test_dataset.obscene == -1) |\n",
    "                                              (test_dataset.threat == -1) |\n",
    "                                              (test_dataset.insult == -1) |\n",
    "                                              (test_dataset.identity_hate == -1)].index)\n",
    "train_dataset = pd.read_pickle(\"balanced_dataset.pickle\")\n",
    "\n",
    "print(train_dataset.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "periodic-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "regex_str = \"[^a-zA-Z\\s]\"\n",
    "train_dataset['comment_text'] = train_dataset['comment_text'].replace(regex=regex_str, value=\"\")\n",
    "\n",
    "# Remove extra whitespaces\n",
    "regex_space = \"\\s+\"\n",
    "train_dataset['comment_text'] = train_dataset['comment_text'].replace(regex=regex_space, value=\" \")\n",
    "\n",
    "# Strip whitespaces\n",
    "train_dataset['comment_text'] = train_dataset['comment_text'].str.strip()\n",
    "\n",
    "# Lowercase\n",
    "train_dataset['comment_text'] = train_dataset['comment_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efficient-ballet",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def tokenize(text):\n",
    "    return [word_tokenize(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ongoing-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert comment_text column into a list\n",
    "comment_list = train_dataset['comment_text'].tolist()\n",
    "\n",
    "# Tokenize\n",
    "comment_token = tokenize(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "driven-receiver",
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8419eaf21399>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Create bigram model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbigram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mbigram_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhraser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\grp7_env\\lib\\site-packages\\gensim\\models\\phrases.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, phrases_model)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'source_vocab length %i'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrases_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbigram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mphrases_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_phrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_tuples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbigram\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphrasegrams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Phraser repeat %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\grp7_env\\lib\\site-packages\\gensim\\models\\phrases.py\u001b[0m in \u001b[0;36mexport_phrases\u001b[1;34m(self, sentences, out_delimiter, as_tuples)\u001b[0m\n\u001b[0;32m    628\u001b[0m             \u001b[1;31m# keeps only not None scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m             \u001b[0mfiltered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbigrams\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mas_tuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\grp7_env\\lib\\site-packages\\gensim\\models\\phrases.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[0mbigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyze_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m             \u001b[1;31m# keeps only not None scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 629\u001b[1;33m             \u001b[0mfiltered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbigrams\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mas_tuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\grp7_env\\lib\\site-packages\\gensim\\models\\phrases.py\u001b[0m in \u001b[0;36manalyze_sentence\u001b[1;34m(self, sentence, threshold, common_terms, scorer)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0manalyze_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommon_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m         \"\"\"Analyze a sentence, detecting any bigrams that should be concatenated.\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Gensim N-grams\n",
    "# Create bigram model\n",
    "bigram = Phrases(comment_token, min_count=5, threshold=100)\n",
    "bigram_model = Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(comment_token):\n",
    "\n",
    "    temp_comment = []\n",
    "\n",
    "    for comment in comment_token:\n",
    "        \n",
    "        temp_word = []\n",
    "\n",
    "        for word in comment:\n",
    "            \n",
    "            if word not in STOP_WORDS:\n",
    "                temp_word.append(word)\n",
    "\n",
    "        temp_comment.append(temp_word)\n",
    "    \n",
    "    return temp_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['cocksucker', 'piss', 'around', 'work'], ['gay', 'antisemmitian', 'archangel', 'white', 'tiger', 'meow', 'greetingshhh', 'uh', 'two', 'ways', 'erased', 'comment', 'ww', 'holocaust', 'brutally', 'slaying', 'jews', 'gaysgypsysslavsanyone', 'antisemitian', 'shave', 'head', 'bald', 'go', 'skinhead', 'meetings', 'doubt', 'words', 'bible', 'homosexuality', 'deadly', 'sin', 'make', 'pentagram', 'tatoo', 'forehead', 'go', 'satanistic', 'masses', 'gay', 'pals', 'first', 'last', 'warning', 'fucking', 'gay', 'wont', 'appreciate', 'nazi', 'shwain', 'would', 'write', 'page', 'dont', 'wish', 'talk', 'anymore', 'beware', 'dark', 'side'], ['stupid', 'peace', 'shit', 'stop', 'deleting', 'stuff', 'asshole', 'go', 'die', 'fall', 'hole', 'go', 'hell']]\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "comment_stop = remove_stopwords(comment_token)\n",
    "print(comment_stop[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "introductory-bacon",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Gensim n-grams\n",
    "comment_bigrams = [bigram_model[word] for word in comment_token_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fitting-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    # now we need to convert from nltk to wordnet POS notations (for compatibility reasons)\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN) # return and default to noun if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "complicated-speed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vinal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "comment_lemma = []\n",
    "for comment in comment_token_stop:\n",
    "    temp = []\n",
    "    temp.append([lemmatizer.lemmatize(word, pos=get_wordnet_pos(word)) for word in comment])\n",
    "    comment_lemma += temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "canadian-stewart",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7132\n"
     ]
    }
   ],
   "source": [
    "print(len(comment_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "external-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lemmatised tokens\n",
    "pickle.dump(comment_lemma, open(\"comment_lemma.pickle\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd0c8de7daaf6551741ca0923be7e98795069d9a1942406e7e8abaf0a7da2ca837d",
   "display_name": "Python 3.8.8 64-bit ('grp7_env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}