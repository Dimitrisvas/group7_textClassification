{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the explanation for Task 1, as well as the .ipynb implementation of model.py, and testing the model pipeline works as intended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Research different model serving option(s)and explain what would be the right choice for your case\n",
    "\n",
    "Here we are deploying a machine learning model using Flask. What we mean by deploying is to integrate the machine learning model we have created into an existing production environment where it can take in an input, which in our case is a comment, and an output, which for us is the predicted labels for that comment.\n",
    "\n",
    "When deploying a model there are a number of production-related issues we need to consider. We need to consider how the model deals with high traffic or how to deal with storage and management of different versions of the ML model. There are many different ways in which to serve our model but some of these options may not solve these common production-related problems we have, such as the 'model as code' approach which does not consider these production-related issues. As these problems are very common there are general-purpose platforms for serving and deploying ML models. \n",
    "\n",
    "There are two serving types that these serving options fall into:\n",
    "- Model as code: This is the most common way to deploy a model. A trained model is saved in a binary format to then be wrapped in a microservice such as Flask.\n",
    "- Model as data: Standardizing the model format as to be usable by any programming language meaning you do not need to wrap it in a microservice. By using this approach we can solve some of these production-related issues. \n",
    "\n",
    "The advantages of using model as code is that it simplifies the deployment process and can provide the tools to perform canary releases and A/B testing. The issue of using 'model as code' approach is that as the number of models grows, the number of microservices multiplies, increasing the number of failure points and making it difficult to manage. Model as data due to the model being called directly means that we do not need to worry about monitoring or error handling. \n",
    "\n",
    "As we will only be having a singular model it means that we do not need to worry about the issue of a multiplying microservices. As using the 'model as code' approach simplifies the deployment process and gives us the tools to test our service it means it is well suited for this project. I will be comparing Tensorflow serving which uses the 'model as data' approach to Flask which uses the 'model as code' approach to decide what is best suited  for this project and what serving type is best for this project. I will comparing a third option too that being Django.\n",
    "\n",
    "From what was discussed above our group decided to research into these following options when deploying and serving our model, those are Flask, Django, and Tensorflow. We have a lot of experience with TensorFlow and itâ€™s most suitable for updating models when needed, good for batch requests, can use gpu. The issue with Tensorflow serving is that it is unable to support sklearn making it unviable for serving our model. Django is is more powerful and scalable when compared to flask and is good for larger applications. Overall When looking between Flask and Django we came to the conclusion that Flask was best suited for our model as it is easy to use, minimalistic with no restrictions, and Flask being better suited for simple websites which in our case is great as our web application is quite simple as all it will do is take in a comment and predict what labels for it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up and Creating the Model Pipeline\n",
    "First, we prepare the data accordingly with the individual experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "\n",
    "# Multinomial Naive Bayes model file path\n",
    "MODEL_DIR = \"multi_mnb_model.joblib\"\n",
    "\n",
    "# Balanced datasets\n",
    "BALANCED_TRAIN_DATASET = \"../balanced_dataset.pickle\"\n",
    "\n",
    "# Preprocessed balanced data\n",
    "PREPROCESSED_BAL_TRAIN_DATASET = \"../preprocessed_train.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load pickle file\n",
    "# Params:\n",
    "    # Str - @file_path: File path of pickle file\n",
    "# Output:\n",
    "    # Saved object in original file type (list/dataframe)\n",
    "def load_pickle(file_path):\n",
    "    return pickle.load(open(file_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get preprocessed train dataset\n",
    "bal_train_dataset = load_pickle(PREPROCESSED_BAL_TRAIN_DATASET)\n",
    "\n",
    "# Get train_y\n",
    "bal_train_y = pd.read_pickle(BALANCED_TRAIN_DATASET)\n",
    "bal_train_y = bal_train_y.drop(columns=\"comment_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for model pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing imports\n",
    "from functools import lru_cache\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy function for TfidfVectorizer tokenizer\n",
    "def fake_function(comments):\n",
    "    return comments\n",
    "\n",
    "# Pre-processing functions\n",
    "\n",
    "\n",
    "# Function to clean comments in dataset\n",
    "# Params: \n",
    "#   Pandas dataframe - @dataset: Data to be cleaned\n",
    "# Output: \n",
    "#   List    - @comment_list: Cleaned comments (2D List)\n",
    "def clean_data(dataset):\n",
    "\n",
    "    # Remove punctuation\n",
    "    regex_str = \"[^a-zA-Z\\s]\"\n",
    "    dataset['comment_text'] = dataset['comment_text'].replace(regex=regex_str, value=\"\")\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    regex_space = \"\\s+\"\n",
    "    dataset['comment_text'] = dataset['comment_text'].replace(regex=regex_space, value=\" \")\n",
    "\n",
    "    # Strip whitespaces\n",
    "    dataset['comment_text'] = dataset['comment_text'].str.strip()\n",
    "\n",
    "    # Lowercase\n",
    "    dataset['comment_text'] = dataset['comment_text'].str.lower()\n",
    "\n",
    "    # Convert comment_text column into a list\n",
    "    comment_list = dataset['comment_text'].tolist()\n",
    "\n",
    "    return comment_list\n",
    "\n",
    "# Function to get NLTK POS Tagger\n",
    "# Params: \n",
    "#   Str - @word: Token\n",
    "# Output\n",
    "#   Dict - POS tagger\n",
    "def nltk_get_wordnet_pos(word):\n",
    "    \n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "\n",
    "    # Convert NLTK to wordnet POS notations\n",
    "\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN) # Default to noun if not found\n",
    "\n",
    "# Function to use NLTK lemmatizer\n",
    "# Params: 2D List - Tokenized comments with stopwords removed\n",
    "# Returns: 2D List - lemmatized tokens\n",
    "def nltk_lemmatize(comment_stop):\n",
    "\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    comment_lemma = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizer_cache = lru_cache(maxsize=50000)(lemmatizer.lemmatize)\n",
    "\n",
    "    for comment in comment_stop:\n",
    "        temp = []\n",
    "        temp.append([lemmatizer_cache(word, pos=nltk_get_wordnet_pos(word)) for word in comment])\n",
    "        comment_lemma += temp\n",
    "\n",
    "    return comment_lemma\n",
    "\n",
    "# Function to remove NLTK stopwords\n",
    "# Params: \n",
    "#   2D List - @comment_token:   cleaned & tokenized comments\n",
    "# Output:\n",
    "#   2D List - @comment_stop: cleaned tokens with stopwords removed\n",
    "def nltk_stopwords(comment_token):\n",
    "    # Stopwords in English only\n",
    "    STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "    # Remove stopwords\n",
    "    comment_stop = []\n",
    "\n",
    "    for comment in comment_token:\n",
    "        \n",
    "        temp_word = []\n",
    "\n",
    "        for word in comment:\n",
    "            \n",
    "            if word not in STOP_WORDS:\n",
    "                temp_word.append(word)\n",
    "\n",
    "        comment_stop.append(temp_word)\n",
    "\n",
    "    return comment_stop\n",
    "\n",
    "# Function to tokenize comments using NLTK Word Tokenize\n",
    "# Params: \n",
    "#   2D List - @text: cleaned comments\n",
    "# Output: \n",
    "#   2D List - tokenized comments\n",
    "def nltk_tokenize(text):\n",
    "    return [word_tokenize(word) for word in text]\n",
    "\n",
    "# Function for all pre-processing functions without saving as pickle file\n",
    "# Params:\n",
    "#   List  - @dataset: Dataset to be pre-processed (train/test)\n",
    "# Output:\n",
    "#   List - @comments_list: Preprocessed tokens (2D List)\n",
    "def preprocess_data_without_pickle(dataset):\n",
    "\n",
    "    # Prevent re-running on already preprocessed data\n",
    "    if isinstance(dataset, pd.DataFrame): #if dataframe, data isn't preprocessed\n",
    "\n",
    "        comments_list = clean_data(dataset)\n",
    "        \n",
    "        # NLTK Tokenize\n",
    "        comments_list = nltk_tokenize(comments_list)\n",
    "\n",
    "        # Remove NLTK stopwords\n",
    "        comments_list = nltk_stopwords(comments_list)\n",
    "\n",
    "        # NLTK Lemmatization\n",
    "        comments_list = nltk_lemmatize(comments_list)\n",
    "\n",
    "        return comments_list\n",
    "    \n",
    "    else:\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the pipeline with TfidfVectorizer and our chosen classifier, Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(min_df=5,\n",
       "                                 preprocessor=<function preprocess_data_without_pickle at 0x00000227B66CC550>,\n",
       "                                 sublinear_tf=True, token_pattern=None,\n",
       "                                 tokenizer=<function fake_function at 0x00000227AFA2AE50>)),\n",
       "                ('multi_mnb',\n",
       "                 MultiOutputClassifier(estimator=MultinomialNB(), n_jobs=-1))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the pipeline with TfidfVectorizer and Multinomial Naive Bayes\n",
    "# Pass in dummy function into TfidfVectorizer's tokenizer\n",
    "# Pass in our custom preprocess function into TfidfVectorizer's preprocesser\n",
    "# Create Multinomial Naive Bayes MultiOutputClassifier model\n",
    "pipe = Pipeline([ \n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        analyzer='word', \n",
    "        tokenizer=fake_function, \n",
    "        preprocessor=preprocess_data_without_pickle, \n",
    "        token_pattern=None,\n",
    "        min_df=5, \n",
    "        norm='l2', \n",
    "        smooth_idf=True, \n",
    "        sublinear_tf=True)), \n",
    "    ('multi_mnb', MultiOutputClassifier(MultinomialNB(), n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipe.fit(bal_train_dataset, bal_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multi_mnb_model_test.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the pipeline\n",
    "joblib.dump(pipe, 'multi_mnb_model_test.joblib', compress=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Pipeline\n",
    "The following code tests the model pipeline against the functions created for the web application to ensure the model works as intended, such as ensuring the comment is in a form that can be used by the model (dataframe instead of string). We are also ensuring that the pipeline returns predictions in a format that is expected (e.g. binary outputs instead of probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "pipe = joblib.load('multi_mnb_model_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
      "       'identity_hate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# List of columns for dataframes (temp and global)\n",
    "cols = ['comment_text','toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "labels = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "df = pd.DataFrame(columns=cols)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to df to feed into pipeline\n",
    "# Params:\n",
    "#   String - @comment: Input from form in web app\n",
    "# Output:\n",
    "#   Dataframe - @temp_df: Temporary dataframe of a single comment to be preprocessed by pipeline\n",
    "def convert_for_pred(comment):\n",
    "\n",
    "    temp_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    new_row = {'comment_text':comment}\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        new_row[labels[i]] = 0\n",
    "\n",
    "    temp_df = temp_df.append(new_row, ignore_index=True)\n",
    "\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           comment_text toxic severe_toxic obscene threat insult identity_hate\n",
      "0  sad fuck one two a b     0            0       0      0      0             0\n",
      "[[1, 1, 1, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Sample test comment\n",
    "test = \"sad fuck one two a b\"\n",
    "\n",
    "comment = convert_for_pred(test)\n",
    "print(comment)\n",
    "\n",
    "# Predict\n",
    "prediction = pipe.predict(comment['comment_text']).tolist()\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        comment_text toxic severe_toxic  \\\n",
      "0             comment_text toxic severe_toxic obs...     1            1   \n",
      "\n",
      "  obscene threat insult identity_hate  \n",
      "0       1      0      1             0  \n"
     ]
    }
   ],
   "source": [
    "# Append comment to global dataframe\n",
    "new_row = {'comment_text':comment}\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    new_row[labels[i]] = prediction[0][i]\n",
    "\n",
    "    #append row to the dataframe\n",
    "df = df.append(new_row, ignore_index=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Discuss the performance of the service you implemented,and justify the good and bad points\n",
    "\n",
    "Our service is able to take in a comment and then predict what labels it has. All comments are then saved and displayed along with its date it was made on and the labels assigned to it. It is able to do all of this pretty quickly and does not have any errors. Our service is able to take in user inputs and store the data into a database to then be displayed pretty well. The service is also able to actually predict labels to comments showing that the deployment and serving of the model is successful. It is able to do all of this well because of flask was implemented well. The site is able to handle inputs well too. The issue however is with the model and its predictions as it will assign comments that are not toxic such as hello and give it a label that is toxic. The labels most commonly assigned to these non-toxic messages is the toxic label meaning our model has a poor predictive performance when dealing with such comments. This is caused by the model itself not assigning the proper label which can be caused by a multitude of different reasons such as not employing the best data balancing strategy or not preprocessing the training well enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Solution Deployment\n",
    "As outlined above, we have opted to implement the code required to build and deploy the model using Python code, across files such as **model.py** and **prediction.py**, called by **\\__init\\__.py**. For this reason, the only unfulfilled requirement is preparing the environment before deployment. \n",
    "\n",
    "We have opted to use MLflow for this purpose, as it requires the least effort to implement, and features great compatibility with Anaconda environments, which we have already been using in our project development process.\n",
    "\n",
    "The file **group7_textClassification/MLproject** specifies the details of the deployment project. It points to **grp7_env.yml** to identify project dependencies to retrieve, and creates a conda environment for the project. Once a project has been run once, its environment will only need to be handled when updates to the .yml file are made. It will then run the **\\__init\\__.py** file, which will in turn call all relevant .py files in order to build and deploy the model at **http://localhost:5000**.\n",
    "\n",
    "The MLproject may be run using the Anaconda CLI after installing MLflow, by navigating to the **group7_textClassification** folder and running the command **'mlflow run .'**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
