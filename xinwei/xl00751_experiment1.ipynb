{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0c8de7daaf6551741ca0923be7e98795069d9a1942406e7e8abaf0a7da2ca837d",
   "display_name": "Python 3.8.8 64-bit ('grp7_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Experiment 1\n",
    "## Comparing Multiple Lemmatizations\n",
    "In this experiment, I have successfully used NLTK and spaCy lemmatizers. I had attempted to use Gensim, TreeTaggerWrapper and TextBlob as well but ran into various issues.\n",
    "\n",
    "Gensim's lemmatizer relied on Pattern, but Pattern has not been updated for several years, so Gensim has removed their lemmatize function from Gensnim 4.0. The discussion can be found here: https://github.com/RaRe-Technologies/gensim/issues/2716\n",
    "\n",
    "TreeTaggerWrapper required multiple local installations to use compared to a simple pip install command which other libraries use. First their package, then their tagging scripts, followed by installation scripts and finally parameter files. I was concerned that due to a multi-step installation, it would not work properly when run on a different computer. (https://www.cis.lmu.de/~schmid/tools/TreeTagger/)\n",
    "\n",
    "Lastly, TextBlob made use of TextBlob and Word objects. TextBlob only takes in non-tokenized Strings and Word only takes in a single word String. To use TextBlob, I would have to use untokenized text which would not make for a fair experiment as NLTK and spaCy performed lemmatizations on sentences tokenized by the same library (NLTK). While documentation states that they have WordList objects which contains Words in a TextBlob, WordLists do not have contain a pos tags parameter which makes me unable to use it for lemmatization. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "\n",
    "# Data Directory\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# Balanced datasets\n",
    "BALANCED_TRAIN_DATASET = \"data/balanced_dataset.pickle\"\n",
    "BALANCED_TEST_DATASET = \"data/balanced_test_dataset.pickle\"\n",
    "\n",
    "# Preprocessed balanced test dataset\n",
    "PREPROCESSED_BAL_TEST_DATASET = \"data/preprocessed_test.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save data as a .pickle file\n",
    "# Params: \n",
    "    # List or Dataframe - @data: Data to be saved as pickle\n",
    "    # Str - @folder: folder name\n",
    "    # Str - file name\n",
    "# Output: Pickle file in directory/repo \n",
    "def save_pickle(data, folder, file_name):\n",
    "    with open(\"{0}/{1}.pickle\".format(folder, file_name), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Saved data is stored in \\'{folder}\\' in the form of {file_name}.pickle\")\n",
    "    #pickle.dump(data, open(\"data/{0}.pickle\".format(file_name),\"wb\"))\n",
    "\n",
    "# Function to load pickle file\n",
    "# Params:\n",
    "    # Str - @file_path: File path of pickle file\n",
    "# Output:\n",
    "    # Saved object in original file type (list/dataframe)\n",
    "def load_pickle(file_path):\n",
    "    return pickle.load(open(file_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "# Balanced, unprocessed datasets\n",
    "bal_train_df = load_pickle(BALANCED_TRAIN_DATASET)\n",
    "bal_test_df = load_pickle(BALANCED_TEST_DATASET)\n",
    "\n",
    "# Get test dataset\n",
    "bal_test_dataset = load_pickle(PREPROCESSED_BAL_TEST_DATASET)\n",
    "\n",
    "# Get train_y\n",
    "bal_train_y = pd.read_pickle(BALANCED_TRAIN_DATASET)\n",
    "bal_train_y = bal_train_y.drop(columns=\"comment_text\")\n",
    "\n",
    "# Get test_y\n",
    "bal_test_y = pd.read_pickle(BALANCED_TEST_DATASET)\n",
    "bal_test_y = bal_test_y.drop(columns=\"comment_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing imports\n",
    "import functools\n",
    "import nltk\n",
    "\n",
    "from functools import lru_cache\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing functions\n",
    "\n",
    "# Function to clean comments in train dataset\n",
    "# Params: \n",
    "#   pd dataframe    - @train_dataset: Training dataset\n",
    "# Output: \n",
    "#   2D List         - @comment_list: cleaned comments\n",
    "def clean_data(train_dataset):\n",
    "    # Remove punctuation\n",
    "    regex_str = \"[^a-zA-Z\\s]\"\n",
    "    train_dataset['comment_text'] = train_dataset['comment_text'].replace(regex=regex_str, value=\"\")\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    regex_space = \"\\s+\"\n",
    "    train_dataset['comment_text'] = train_dataset['comment_text'].replace(regex=regex_space, value=\" \")\n",
    "\n",
    "    # Strip whitespaces\n",
    "    train_dataset['comment_text'] = train_dataset['comment_text'].str.strip()\n",
    "\n",
    "    # Lowercase\n",
    "    train_dataset['comment_text'] = train_dataset['comment_text'].str.lower()\n",
    "\n",
    "    # Convert comment_text column into a list\n",
    "    comment_list = train_dataset['comment_text'].tolist()\n",
    "\n",
    "    return comment_list\n",
    "\n",
    "# Function to get NLTK POS Tagger\n",
    "# Params: \n",
    "#   Str - @word: Token\n",
    "# Output\n",
    "#   Dict - POS tagger\n",
    "def nltk_get_wordnet_pos(word):\n",
    "    \n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "\n",
    "    # Convert NLTK to wordnet POS notations\n",
    "\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN) # Default to noun if not found\n",
    "\n",
    "# Function to use NLTK lemmatizer\n",
    "# Params: 2D List - Tokenized comments with stopwords removed\n",
    "# Returns: 2D List - lemmatized tokens\n",
    "def nltk_lemmatize(comment_stop):\n",
    "\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    comment_lemma = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizer_cache = lru_cache(maxsize=50000)(lemmatizer.lemmatize)\n",
    "\n",
    "    for comment in comment_stop:\n",
    "        temp = []\n",
    "        temp.append([lemmatizer_cache(word, pos=nltk_get_wordnet_pos(word)) for word in comment])\n",
    "        comment_lemma += temp\n",
    "\n",
    "    return comment_lemma\n",
    "\n",
    "# Function to remove NLTK stopwords\n",
    "# Params: \n",
    "#   2D List - @comment_token: cleaned & tokenized comments\n",
    "# Output:\n",
    "#   2D List - @comment_stop: cleaned tokens with stopwords removed\n",
    "def nltk_stopwords(comment_token):\n",
    "    # Stopwords in English only\n",
    "    STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "    # Remove stopwords\n",
    "    comment_stop = []\n",
    "\n",
    "    for comment in comment_token:\n",
    "        \n",
    "        temp_word = []\n",
    "\n",
    "        for word in comment:\n",
    "            \n",
    "            if word not in STOP_WORDS:\n",
    "                temp_word.append(word)\n",
    "\n",
    "        comment_stop.append(temp_word)\n",
    "    \n",
    "    return comment_stop\n",
    "\n",
    "# Function to tokenize comments using NLTK Word Tokenize\n",
    "# Params: \n",
    "#   2D List - @text: cleaned comments\n",
    "# Output: \n",
    "#   2D List - tokenized comments\n",
    "def nltk_tokenize(text):\n",
    "    return [word_tokenize(word) for word in text]\n",
    "\n",
    "# Function for all pre-processing functions\n",
    "# Params:\n",
    "    # Pandas Dataframe  - @dataset: Dataset to be pre-processed (train/test)\n",
    "    # Str               - @file_name: File name to save pre-processed data as pickle\n",
    "# Output: Pickle file in directory/repo\n",
    "def preprocess_data(dataset, file_name):\n",
    "\n",
    "    comment_cleaned = clean_data(dataset)\n",
    "    \n",
    "    # NLTK Tokenize\n",
    "    comment_token = nltk_tokenize(comment_cleaned)\n",
    "\n",
    "    # Remove NLTK stopwords\n",
    "    comment_stop = nltk_stopwords(comment_token)\n",
    "\n",
    "    # NLTK Lemmatization\n",
    "    comment_lemma = nltk_lemmatize(comment_stop)\n",
    "\n",
    "    save_pickle(comment_lemma, folder, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare basic pre-processing steps until before lemmatization\n",
    "\n",
    "# Train dataset\n",
    "train_clean = clean_data(bal_train_df)\n",
    "train_token = nltk_tokenize(train_clean)\n",
    "train_stopwords = nltk_stopwords(train_token)\n",
    "\n",
    "# Test dataset\n",
    "test_clean = clean_data(bal_test_df)\n",
    "test_token = nltk_tokenize(test_clean)\n",
    "test_stopwords = nltk_stopwords(test_clean)"
   ]
  },
  {
   "source": [
    "# NLTK Lemmatization\n",
    "The tokens have already been lemmatized by NLTK in basic pre-processing, but since it is a method of lemmatization, I am including it in the experiment.\n",
    "\n",
    "NLTK requires each token to be tagged to a pos tag. NLTK uses WordNetLemmatizer, so it gets pos tags from WordNet. We are required to create a dictionary to convert NLTK's own pos tags to WordNet's equivalent."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get NLTK POS Tagger\n",
    "# Params: \n",
    "#   Str - @word: Token\n",
    "# Output\n",
    "#   Dict - POS tagger\n",
    "def nltk_get_wordnet_pos(word):\n",
    "    \n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "\n",
    "    # Convert NLTK to wordnet POS notations\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN) # Default to noun if not found\n",
    "\n",
    "# Function to use NLTK lemmatizer\n",
    "# Params: 2D List - Tokenized comments with stopwords removed\n",
    "# Returns: 2D List - lemmatized tokens\n",
    "def nltk_lemmatize(comment_stop):\n",
    "\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    comment_lemma = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizer_cache = lru_cache(maxsize=50000)(lemmatizer.lemmatize)\n",
    "\n",
    "    for comment in comment_stop:\n",
    "        temp = []\n",
    "        temp.append([lemmatizer_cache(word, pos=nltk_get_wordnet_pos(word)) for word in comment])\n",
    "        comment_lemma += temp\n",
    "\n",
    "    return comment_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lamxw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[['cocksucker', 'piss', 'around', 'work'], ['gay', 'antisemmitian', 'archangel', 'white', 'tiger', 'meow', 'greetingshhh', 'uh', 'two', 'way', 'erase', 'comment', 'ww', 'holocaust', 'brutally', 'slay', 'jew', 'gaysgypsysslavsanyone', 'antisemitian', 'shave', 'head', 'bald', 'go', 'skinhead', 'meeting', 'doubt', 'word', 'bible', 'homosexuality', 'deadly', 'sin', 'make', 'pentagram', 'tatoo', 'forehead', 'go', 'satanistic', 'mass', 'gay', 'pal', 'first', 'last', 'warn', 'fuck', 'gay', 'wont', 'appreciate', 'nazi', 'shwain', 'would', 'write', 'page', 'dont', 'wish', 'talk', 'anymore', 'beware', 'dark', 'side']]\n"
     ]
    }
   ],
   "source": [
    "# Get comments lemmatized with NLTK to compare\n",
    "comments_nltk = nltk_lemmatize(train_stopwords)\n",
    "print(comments_nltk[:2])"
   ]
  },
  {
   "source": [
    "# Spacy Lemmatization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load small version of spacy's language model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"textcat\"], exclude=[\"parser\", \"ner\"])"
   ]
  },
  {
   "source": [
    "Since spacy tokenizes sentences automatically with their model, I have to replace their tokenizer with our used tokenizer (nltk) to ensure spacy's tokenizing function does not affect our result. I will need to modify nltk_tokenize() to return a spacy Doc object."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create each tokenized comment as a spacy Doc object, then add it to a list\n",
    "# Params:\n",
    "#   2D List - @comment_token: Tokenized comments with stopwords removed\n",
    "# Output:\n",
    "#   List    - @doc_list: List of Doc objects\n",
    "def create_docs(comment_stopwords):\n",
    "\n",
    "    doc_list = []\n",
    "\n",
    "    for comment in comment_stopwords:\n",
    "\n",
    "        single_comment = []\n",
    "\n",
    "        for word in comment:\n",
    "            single_comment.append(word)\n",
    "\n",
    "        doc_list.append(Doc(nlp.vocab, single_comment))\n",
    "        single_comment.clear()\n",
    "    \n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7132\n"
     ]
    }
   ],
   "source": [
    "# Create list of Docs\n",
    "doc_list = create_docs(train_stopwords)\n",
    "print(len(doc_list))"
   ]
  },
  {
   "source": [
    "spaCy lemmatization has a known bug for a specific word 'first' and returns it as a unicode character. I am unsure if any other words are affected by a similar bug.\n",
    "\n",
    "Reference: https://github.com/explosion/spaCy/issues/6281\n",
    "\n",
    "I have chosen to workaround it by hardcoding it to 'first'."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform lemmatization with spaCy\n",
    "# Params:\n",
    "#   List    - @doc_list: List of Doc objects\n",
    "# Output:\n",
    "#   2D List - @comment_list: List of lemmatized tokens\n",
    "def spacy_lemmatize(doc_list):\n",
    "\n",
    "    comment_list = []\n",
    "\n",
    "    for doc in doc_list:\n",
    "\n",
    "        token_list = []\n",
    "\n",
    "        for token in doc:\n",
    "\n",
    "            lemma = token.lemma_\n",
    "\n",
    "            # Hardcode '1' to 'first' to workaround spaCy bug\n",
    "            if ord(lemma[0]) > 127:\n",
    "                lemma = 'first'\n",
    "                token_list.append(lemma)\n",
    "\n",
    "            # spaCy lemmatization returns pronouns as '-PRON-', so exclude it\n",
    "            elif lemma != '-PRON-':\n",
    "                token_list.append(lemma)\n",
    "        \n",
    "        comment_list.append(token_list)\n",
    "    \n",
    "    return comment_list"
   ]
  },
  {
   "source": [
    "I have attempted to resolve it by encoding/decoding it but it always resulted in '\\ufeff1' despite printing '1' during my attempts. I have shown evidence of attempting it with 'convert_tokens()'. The following cell shows my attempt."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['cocksucker', 'piss', 'around', 'work'], ['gay', 'antisemmitian', 'archangel', 'white', 'tiger', 'meow', 'greetingshhh', 'uh', 'two', 'way', 'erase', 'comment', 'ww', 'holocaust', 'brutally', 'slay', 'jews', 'gaysgypsysslavsanyone', 'antisemitian', 'shave', 'head', 'bald', 'go', 'skinhead', 'meeting', 'doubt', 'word', 'bible', 'homosexuality', 'deadly', 'sin', 'make', 'pentagram', 'tatoo', 'forehead', 'go', 'satanistic', 'masse', 'gay', 'pal', '\\ufeff1', 'last', 'warn', 'fuck', 'gay', 'wont', 'appreciate', 'nazi', 'shwain', 'would', 'write', 'page', 'do', 'wish', 'talk', 'anymore', 'beware', 'dark', 'side']]\n[['cocksucker', 'piss', 'around', 'work'], ['gay', 'antisemmitian', 'archangel', 'white', 'tiger', 'meow', 'greetingshhh', 'uh', 'two', 'way', 'erase', 'comment', 'ww', 'holocaust', 'brutally', 'slay', 'jews', 'gaysgypsysslavsanyone', 'antisemitian', 'shave', 'head', 'bald', 'go', 'skinhead', 'meeting', 'doubt', 'word', 'bible', 'homosexuality', 'deadly', 'sin', 'make', 'pentagram', 'tatoo', 'forehead', 'go', 'satanistic', 'masse', 'gay', 'pal', '\\ufeff1', 'last', 'warn', 'fuck', 'gay', 'wont', 'appreciate', 'nazi', 'shwain', 'would', 'write', 'page', 'do', 'wish', 'talk', 'anymore', 'beware', 'dark', 'side']]\n"
     ]
    }
   ],
   "source": [
    "# Perform lemmatization with spaCy\n",
    "# Params:\n",
    "#   List    - @doc_list: List of Doc objects\n",
    "# Output:\n",
    "#   2D List - @comment_list: List of lemmatized tokens\n",
    "def spacy_lemmatize_test(doc_list):\n",
    "\n",
    "    comment_list = []\n",
    "\n",
    "    for doc in doc_list:\n",
    "\n",
    "        token_list = []\n",
    "\n",
    "        #token_list.append([token.lemma_ for token in doc if token.lemma != '-PRON'])\n",
    "\n",
    "        for token in doc:\n",
    "\n",
    "            lemma = token.lemma_\n",
    "\n",
    "            # spaCy lemmatization returns pronouns as '-PRON-', so exclude it\n",
    "            if lemma != '-PRON-':\n",
    "                token_list.append(lemma)\n",
    "        \n",
    "        comment_list.append(token_list)\n",
    "    \n",
    "    return comment_list\n",
    "\n",
    "# Encode and decode unicode bug\n",
    "# Does not work\n",
    "# Params:\n",
    "#   List    - @spacy_list: List of spaCy lemmatized tokens\n",
    "# Output:\n",
    "#   2D List - @comment_list: List of decoded tokens as required\n",
    "def convert_tokens(spacy_list):\n",
    "\n",
    "    comment_list = []\n",
    "\n",
    "    for doc in spacy_list:\n",
    "\n",
    "        token_list = []\n",
    "\n",
    "        for token in doc:\n",
    "            \n",
    "            # If unicode\n",
    "            if ord(token[0]) > 127:\n",
    "\n",
    "                token = token.encode('utf-8')\n",
    "                token = token.decode('utf-8')\n",
    "                token_list.append(token)\n",
    "            \n",
    "            else:\n",
    "                token_list.append(token)\n",
    "        \n",
    "        comment_list.append(token_list)\n",
    "    \n",
    "    return comment_list\n",
    "\n",
    "comments_spacy_test = spacy_lemmatize_test(doc_list)\n",
    "print(comments_spacy_test[:2])\n",
    "\n",
    "# Result remains the same\n",
    "comments_spacy_test = convert_tokens(comments_spacy_test)\n",
    "print(comments_spacy_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['cocksucker', 'piss', 'around', 'work'], ['gay', 'antisemmitian', 'archangel', 'white', 'tiger', 'meow', 'greetingshhh', 'uh', 'two', 'way', 'erase', 'comment', 'ww', 'holocaust', 'brutally', 'slay', 'jews', 'gaysgypsysslavsanyone', 'antisemitian', 'shave', 'head', 'bald', 'go', 'skinhead', 'meeting', 'doubt', 'word', 'bible', 'homosexuality', 'deadly', 'sin', 'make', 'pentagram', 'tatoo', 'forehead', 'go', 'satanistic', 'masse', 'gay', 'pal', 'first', 'last', 'warn', 'fuck', 'gay', 'wont', 'appreciate', 'nazi', 'shwain', 'would', 'write', 'page', 'do', 'wish', 'talk', 'anymore', 'beware', 'dark', 'side']]\n"
     ]
    }
   ],
   "source": [
    "comments_spacy = spacy_lemmatize(doc_list)\n",
    "print(comments_spacy[:2])"
   ]
  },
  {
   "source": [
    "## Compare NLTK lemmatization with spaCy's lemmatization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NLTK lemmatization\n[['cocksucker', 'piss', 'around', 'work'], ['gay', 'antisemmitian', 'archangel', 'white', 'tiger', 'meow', 'greetingshhh', 'uh', 'two', 'way', 'erase', 'comment', 'ww', 'holocaust', 'brutally', 'slay', 'jew', 'gaysgypsysslavsanyone', 'antisemitian', 'shave', 'head', 'bald', 'go', 'skinhead', 'meeting', 'doubt', 'word', 'bible', 'homosexuality', 'deadly', 'sin', 'make', 'pentagram', 'tatoo', 'forehead', 'go', 'satanistic', 'mass', 'gay', 'pal', 'first', 'last', 'warn', 'fuck', 'gay', 'wont', 'appreciate', 'nazi', 'shwain', 'would', 'write', 'page', 'dont', 'wish', 'talk', 'anymore', 'beware', 'dark', 'side']]\nspaCy lemmatization\n[['cocksucker', 'piss', 'around', 'work'], ['gay', 'antisemmitian', 'archangel', 'white', 'tiger', 'meow', 'greetingshhh', 'uh', 'two', 'way', 'erase', 'comment', 'ww', 'holocaust', 'brutally', 'slay', 'jews', 'gaysgypsysslavsanyone', 'antisemitian', 'shave', 'head', 'bald', 'go', 'skinhead', 'meeting', 'doubt', 'word', 'bible', 'homosexuality', 'deadly', 'sin', 'make', 'pentagram', 'tatoo', 'forehead', 'go', 'satanistic', 'masse', 'gay', 'pal', 'first', 'last', 'warn', 'fuck', 'gay', 'wont', 'appreciate', 'nazi', 'shwain', 'would', 'write', 'page', 'do', 'wish', 'talk', 'anymore', 'beware', 'dark', 'side']]\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK lemmatization\")\n",
    "print(comments_nltk[:2])\n",
    "\n",
    "print(\"spaCy lemmatization\")\n",
    "print(comments_spacy[:2])"
   ]
  },
  {
   "source": [
    "## Fitting to Keras model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Dropout, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for keras model\n",
    "NUM_WORDS = 20000\n",
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function taken from utils\n",
    "def build_model(num_words):\n",
    "    EPOCHS = 30\n",
    "    INIT_LR = 1e-3\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(num_words, 128))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3))\n",
    "    model.add(Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "    adam = tf.keras.optimizers.Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer=adam,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "source": [
    "# Model for NLTK"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Tensorflow's Tokenizer for text featurization\n",
    "tokenizer = Tokenizer(NUM_WORDS)\n",
    "\n",
    "# Update internal vocabulary\n",
    "tokenizer.fit_on_texts(comments_nltk)\n",
    "\n",
    "# Create an integer index of each word\n",
    "corpus = tokenizer.word_index\n",
    "\n",
    "# Integer to word\n",
    "reverse_corpus = dict(map(reversed, corpus.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn each word into its corresponding integer\n",
    "nltk_train_x = tokenizer.texts_to_sequences(comments_nltk)\n",
    "\n",
    "# Pad sequences\n",
    "nltk_train_x = keras.preprocessing.sequence.pad_sequences(nltk_train_x, MAX_LEN)\n",
    "nltk_train_x = np.array(nltk_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "119/119 [==============================] - 10s 59ms/step - loss: 0.5704 - accuracy: 0.2889\n",
      "Epoch 2/30\n",
      "119/119 [==============================] - 7s 60ms/step - loss: 0.3543 - accuracy: 0.2894\n",
      "Epoch 3/30\n",
      "119/119 [==============================] - 7s 63ms/step - loss: 0.3044 - accuracy: 0.3560\n",
      "Epoch 4/30\n",
      "119/119 [==============================] - 7s 58ms/step - loss: 0.2685 - accuracy: 0.4078\n",
      "Epoch 5/30\n",
      "119/119 [==============================] - 9s 73ms/step - loss: 0.2613 - accuracy: 0.4336\n",
      "Epoch 6/30\n",
      "119/119 [==============================] - 13s 106ms/step - loss: 0.2433 - accuracy: 0.4433\n",
      "Epoch 7/30\n",
      "119/119 [==============================] - 15s 128ms/step - loss: 0.2280 - accuracy: 0.4628\n",
      "Epoch 8/30\n",
      "119/119 [==============================] - 17s 141ms/step - loss: 0.2160 - accuracy: 0.4740\n",
      "Epoch 9/30\n",
      "119/119 [==============================] - 14s 116ms/step - loss: 0.2019 - accuracy: 0.4959\n",
      "Epoch 10/30\n",
      "119/119 [==============================] - 19s 157ms/step - loss: 0.1763 - accuracy: 0.4773\n",
      "Epoch 11/30\n",
      "119/119 [==============================] - 15s 127ms/step - loss: 0.1726 - accuracy: 0.4783\n",
      "Epoch 12/30\n",
      "119/119 [==============================] - 16s 134ms/step - loss: 0.1535 - accuracy: 0.4651\n",
      "Epoch 13/30\n",
      "119/119 [==============================] - 15s 129ms/step - loss: 0.1475 - accuracy: 0.4508\n",
      "Epoch 14/30\n",
      "119/119 [==============================] - 13s 113ms/step - loss: 0.1408 - accuracy: 0.4564\n",
      "Epoch 15/30\n",
      "119/119 [==============================] - 12s 103ms/step - loss: 0.1320 - accuracy: 0.4495\n",
      "Epoch 16/30\n",
      "119/119 [==============================] - 12s 103ms/step - loss: 0.1237 - accuracy: 0.4471\n",
      "Epoch 17/30\n",
      "119/119 [==============================] - 11s 95ms/step - loss: 0.1207 - accuracy: 0.4342\n",
      "Epoch 18/30\n",
      "119/119 [==============================] - 12s 97ms/step - loss: 0.1102 - accuracy: 0.4123\n",
      "Epoch 19/30\n",
      "119/119 [==============================] - 12s 103ms/step - loss: 0.1083 - accuracy: 0.3944\n",
      "Epoch 20/30\n",
      "119/119 [==============================] - 11s 96ms/step - loss: 0.1041 - accuracy: 0.4178\n",
      "Epoch 21/30\n",
      "119/119 [==============================] - 12s 103ms/step - loss: 0.0978 - accuracy: 0.4090\n",
      "Epoch 22/30\n",
      "119/119 [==============================] - 15s 127ms/step - loss: 0.0945 - accuracy: 0.4151\n",
      "Epoch 23/30\n",
      "119/119 [==============================] - 15s 125ms/step - loss: 0.0967 - accuracy: 0.4291\n",
      "Epoch 24/30\n",
      "119/119 [==============================] - 14s 122ms/step - loss: 0.0944 - accuracy: 0.4293\n",
      "Epoch 25/30\n",
      "119/119 [==============================] - 12s 105ms/step - loss: 0.0888 - accuracy: 0.4211\n",
      "Epoch 26/30\n",
      "119/119 [==============================] - 14s 117ms/step - loss: 0.0907 - accuracy: 0.4258\n",
      "Epoch 27/30\n",
      "119/119 [==============================] - 17s 145ms/step - loss: 0.0874 - accuracy: 0.3967\n",
      "Epoch 28/30\n",
      "119/119 [==============================] - 20s 165ms/step - loss: 0.0854 - accuracy: 0.4068\n",
      "Epoch 29/30\n",
      "119/119 [==============================] - 16s 136ms/step - loss: 0.0840 - accuracy: 0.4029\n",
      "Epoch 30/30\n",
      "119/119 [==============================] - 17s 144ms/step - loss: 0.0807 - accuracy: 0.4086\n",
      "INFO:tensorflow:Assets written to: models/exp1_nltk\\assets\n"
     ]
    }
   ],
   "source": [
    "model = build_model(NUM_WORDS)\n",
    "\n",
    "model.fit(nltk_train_x, bal_train_y, batch_size=60, epochs=30)\n",
    "\n",
    "# Save model to use for evaluation\n",
    "model.save('models/exp1_nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(6978, 100)\n"
     ]
    }
   ],
   "source": [
    "# Prepare test_x\n",
    "# No preprocessing needs to be done on the dataset as the base model already made use of NLTK's lemmatization\n",
    "\n",
    "# Turn each word into its corresponding integer\n",
    "nltk_test_x = tokenizer.texts_to_sequences(bal_test_dataset)\n",
    "\n",
    "# Pad sequences\n",
    "nltk_test_x = keras.preprocessing.sequence.pad_sequences(nltk_test_x, MAX_LEN)\n",
    "nltk_test_x = np.array(nltk_test_x)\n",
    "\n",
    "print(nltk_test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "117/117 [==============================] - 2s 11ms/step - loss: 0.9349 - accuracy: 0.3505\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.9349310398101807, 0.3505302369594574]"
      ]
     },
     "metadata": {},
     "execution_count": 269
    }
   ],
   "source": [
    "# Evaluate model for NLTK\n",
    "model.evaluate(nltk_test_x, bal_test_y, batch_size=60)"
   ]
  },
  {
   "source": [
    "# Model for spaCy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Tensorflow's Tokenizer for text featurization\n",
    "tokenizer = Tokenizer(NUM_WORDS)\n",
    "\n",
    "# Update internal vocabulary\n",
    "tokenizer.fit_on_texts(comments_spacy)\n",
    "\n",
    "# Create an integer index of each word\n",
    "corpus = tokenizer.word_index\n",
    "\n",
    "# Integer to word\n",
    "reverse_corpus = dict(map(reversed, corpus.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn each word into its corresponding integer\n",
    "spacy_train_x = tokenizer.texts_to_sequences(comments_spacy)\n",
    "\n",
    "# Pad sequences\n",
    "spacy_train_x = keras.preprocessing.sequence.pad_sequences(spacy_train_x, MAX_LEN)\n",
    "spacy_train_x = np.array(spacy_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "119/119 [==============================] - 11s 73ms/step - loss: 0.5769 - accuracy: 0.4130\n",
      "Epoch 2/30\n",
      "119/119 [==============================] - 7s 57ms/step - loss: 0.3646 - accuracy: 0.3491\n",
      "Epoch 3/30\n",
      "119/119 [==============================] - 7s 58ms/step - loss: 0.2995 - accuracy: 0.3750\n",
      "Epoch 4/30\n",
      "119/119 [==============================] - 7s 59ms/step - loss: 0.2764 - accuracy: 0.4360\n",
      "Epoch 5/30\n",
      "119/119 [==============================] - 7s 56ms/step - loss: 0.2577 - accuracy: 0.4474\n",
      "Epoch 6/30\n",
      "119/119 [==============================] - 7s 56ms/step - loss: 0.2467 - accuracy: 0.4573\n",
      "Epoch 7/30\n",
      "119/119 [==============================] - 7s 57ms/step - loss: 0.2324 - accuracy: 0.4650\n",
      "Epoch 8/30\n",
      "119/119 [==============================] - 7s 58ms/step - loss: 0.2255 - accuracy: 0.4823\n",
      "Epoch 9/30\n",
      "119/119 [==============================] - 7s 61ms/step - loss: 0.2114 - accuracy: 0.4702\n",
      "Epoch 10/30\n",
      "119/119 [==============================] - 6s 54ms/step - loss: 0.1928 - accuracy: 0.4751\n",
      "Epoch 11/30\n",
      "119/119 [==============================] - 7s 60ms/step - loss: 0.1790 - accuracy: 0.4709\n",
      "Epoch 12/30\n",
      "119/119 [==============================] - 7s 62ms/step - loss: 0.1640 - accuracy: 0.4553\n",
      "Epoch 13/30\n",
      "119/119 [==============================] - 6s 53ms/step - loss: 0.1547 - accuracy: 0.4469\n",
      "Epoch 14/30\n",
      "119/119 [==============================] - 6s 53ms/step - loss: 0.1489 - accuracy: 0.4102\n",
      "Epoch 15/30\n",
      "119/119 [==============================] - 6s 52ms/step - loss: 0.1336 - accuracy: 0.3842\n",
      "Epoch 16/30\n",
      "119/119 [==============================] - 6s 53ms/step - loss: 0.1296 - accuracy: 0.3960\n",
      "Epoch 17/30\n",
      "119/119 [==============================] - 7s 58ms/step - loss: 0.1190 - accuracy: 0.3855\n",
      "Epoch 18/30\n",
      "119/119 [==============================] - 7s 56ms/step - loss: 0.1165 - accuracy: 0.3986\n",
      "Epoch 19/30\n",
      "119/119 [==============================] - 8s 70ms/step - loss: 0.1116 - accuracy: 0.3852\n",
      "Epoch 20/30\n",
      "119/119 [==============================] - 7s 63ms/step - loss: 0.1093 - accuracy: 0.3814\n",
      "Epoch 21/30\n",
      "119/119 [==============================] - 7s 57ms/step - loss: 0.1035 - accuracy: 0.3820\n",
      "Epoch 22/30\n",
      "119/119 [==============================] - 7s 55ms/step - loss: 0.1010 - accuracy: 0.3702\n",
      "Epoch 23/30\n",
      "119/119 [==============================] - 7s 58ms/step - loss: 0.0973 - accuracy: 0.3743\n",
      "Epoch 24/30\n",
      "119/119 [==============================] - 7s 61ms/step - loss: 0.0972 - accuracy: 0.3906\n",
      "Epoch 25/30\n",
      "119/119 [==============================] - 6s 53ms/step - loss: 0.0945 - accuracy: 0.4116\n",
      "Epoch 26/30\n",
      "119/119 [==============================] - 6s 53ms/step - loss: 0.0929 - accuracy: 0.3835\n",
      "Epoch 27/30\n",
      "119/119 [==============================] - 7s 57ms/step - loss: 0.0902 - accuracy: 0.3658\n",
      "Epoch 28/30\n",
      "119/119 [==============================] - 7s 59ms/step - loss: 0.0873 - accuracy: 0.3778\n",
      "Epoch 29/30\n",
      "119/119 [==============================] - 7s 58ms/step - loss: 0.0863 - accuracy: 0.3848\n",
      "Epoch 30/30\n",
      "119/119 [==============================] - 14s 114ms/step - loss: 0.0864 - accuracy: 0.3687\n",
      "INFO:tensorflow:Assets written to: models/exp1_spacy\\assets\n"
     ]
    }
   ],
   "source": [
    "model = build_model(NUM_WORDS)\n",
    "\n",
    "model.fit(spacy_train_x, bal_train_y, batch_size=60, epochs=30)\n",
    "\n",
    "# Save model to use for evaluation\n",
    "model.save('models/exp1_spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(6978, 100)\n"
     ]
    }
   ],
   "source": [
    "# Prepare test_x\n",
    "\n",
    "# Pre-process test_x \n",
    "spacy_clean = clean_data(bal_test_df)\n",
    "spacy_tokens = nltk_tokenize(spacy_clean)\n",
    "spacy_stopwords = nltk_stopwords(spacy_tokens)\n",
    "spacy_docs = create_docs(spacy_stopwords)\n",
    "spacy_test_x = spacy_lemmatize(spacy_docs)\n",
    "\n",
    "# Turn each word into its corresponding integer\n",
    "spacy_test_x = tokenizer.texts_to_sequences(spacy_test_x)\n",
    "\n",
    "# Pad sequences\n",
    "spacy_test_x = keras.preprocessing.sequence.pad_sequences(spacy_test_x, MAX_LEN)\n",
    "spacy_test_x = np.array(spacy_test_x)\n",
    "\n",
    "print(nltk_test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "117/117 [==============================] - 1s 5ms/step - loss: 1.0017 - accuracy: 0.3220\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1.0017470121383667, 0.3220120370388031]"
      ]
     },
     "metadata": {},
     "execution_count": 274
    }
   ],
   "source": [
    "# Evaluate model for NLTK\n",
    "model.evaluate(spacy_test_x, bal_test_y, batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}