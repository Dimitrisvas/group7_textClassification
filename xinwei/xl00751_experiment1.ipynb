{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Experiment 1\n",
    "## Comparing Multiple Lemmatizations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "\n",
    "# Data Directory\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# Balanced datasets\n",
    "BALANCED_TRAIN_DATASET = \"data/balanced_dataset.pickle\"\n",
    "BALANCED_TEST_DATASET = \"data/balanced_test_dataset.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save data as a .pickle file\n",
    "# Params: \n",
    "    # List or Dataframe - @data: Data to be saved as pickle\n",
    "    # Str - @folder: folder name\n",
    "    # Str - file name\n",
    "# Output: Pickle file in directory/repo \n",
    "def save_pickle(data, folder, file_name):\n",
    "    with open(\"{0}/{1}.pickle\".format(folder, file_name), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Saved data is stored in \\'{folder}\\' in the form of {file_name}.pickle\")\n",
    "    #pickle.dump(data, open(\"data/{0}.pickle\".format(file_name),\"wb\"))\n",
    "\n",
    "# Function to load pickle file\n",
    "# Params:\n",
    "    # Str - @file_path: File path of pickle file\n",
    "# Output:\n",
    "    # Saved object in original file type (list/dataframe)\n",
    "def load_pickle(file_path):\n",
    "    return pickle.load(open(file_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "# Balanced, unprocessed datasets\n",
    "bal_train_df = load_pickle(BALANCED_TRAIN_DATASET)\n",
    "bal_test_df = load_pickle(BALANCED_TEST_DATASET)\n",
    "\n",
    "# Get train_y\n",
    "bal_train_y = pd.read_pickle(BALANCED_TRAIN_DATASET)\n",
    "bal_train_y = bal_train_y.drop(columns=\"comment_text\")\n",
    "\n",
    "# Get test_y\n",
    "bal_test_y = pd.read_pickle(BALANCED_TEST_DATASET)\n",
    "bal_test_y = bal_test_y.drop(columns=\"comment_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing functions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function to clean comments in train dataset\n",
    "# Params: pd dataframe - Training dataset\n",
    "# Return: 2D List - cleaned comments\n",
    "def clean_data(train_dataset):\n",
    "    # Remove punctuation\n",
    "    regex_str = \"[^a-zA-Z\\s]\"\n",
    "    train_dataset['comment_text'] = train_dataset['comment_text'].replace(regex=regex_str, value=\"\")\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    regex_space = \"\\s+\"\n",
    "    train_dataset['comment_text'] = train_dataset['comment_text'].replace(regex=regex_space, value=\" \")\n",
    "\n",
    "    # Strip whitespaces\n",
    "    train_dataset['comment_text'] = train_dataset['comment_text'].str.strip()\n",
    "\n",
    "    # Lowercase\n",
    "    train_dataset['comment_text'] = train_dataset['comment_text'].str.lower()\n",
    "\n",
    "    # Convert comment_text column into a list\n",
    "    comment_list = train_dataset['comment_text'].tolist()\n",
    "\n",
    "    return comment_list\n",
    "\n",
    "# Function to get NLTK POS Tagger\n",
    "# Params: Token\n",
    "# Returns: Dict - POS tagger\n",
    "def nltk_get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    # Convert NOTK to wordnet POS notations\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN) # Default to noun if not found\n",
    "\n",
    "# Function to use NLTK lemmatizer\n",
    "# Params: 2D List - Tokenized comments with stopwords removed\n",
    "# Returns: 2D List - lemmatized tokens\n",
    "def nltk_lemmatize(comment_stop):\n",
    "\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    comment_lemma = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizer_cache = lru_cache(maxsize=50000)(lemmatizer.lemmatize)\n",
    "\n",
    "    for comment in comment_stop:\n",
    "        temp = []\n",
    "        temp.append([lemmatizer_cache(word, pos=nltk_get_wordnet_pos(word)) for word in comment])\n",
    "        comment_lemma += temp\n",
    "\n",
    "    return comment_lemma\n",
    "\n",
    "# Function to remove NLTK stopwords\n",
    "# Params: 2D List - cleaned & tokenized comments\n",
    "# Returns: 2D List - cleaned tokens with stopwords removed\n",
    "def nltk_stopwords(comment_token):\n",
    "    # Stopwords in English only\n",
    "    STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "    # Remove stopwords\n",
    "    comment_stop = []\n",
    "\n",
    "    for comment in comment_token:\n",
    "        \n",
    "        temp_word = []\n",
    "\n",
    "        for word in comment:\n",
    "            \n",
    "            if word not in STOP_WORDS:\n",
    "                temp_word.append(word)\n",
    "\n",
    "        comment_stop.append(temp_word)\n",
    "    \n",
    "    return comment_stop\n",
    "\n",
    "# Function to tokenize comments using NLTK Word Tokenize\n",
    "# Params: 2D List - cleaned comments\n",
    "# Returns: 2D List - tokenized comments\n",
    "def nltk_tokenize(text):\n",
    "    return [word_tokenize(word) for word in text]\n",
    "\n",
    "# Function for all pre-processing functions\n",
    "# Params:\n",
    "    # Pandas Dataframe  - @dataset: Dataset to be pre-processed (train/test)\n",
    "    # Str               - @file_name: File name to save pre-processed data as pickle\n",
    "# Output: Pickle file in directory/repo\n",
    "def preprocess_data(dataset, file_name):\n",
    "\n",
    "    comment_cleaned = clean_data(dataset)\n",
    "    \n",
    "    # NLTK Tokenize\n",
    "    comment_token = nltk_tokenize(comment_cleaned)\n",
    "\n",
    "    # Remove NLTK stopwords\n",
    "    comment_stop = nltk_stopwords(comment_token)\n",
    "\n",
    "    # NLTK Lemmatization\n",
    "    comment_lemma = nltk_lemmatize(comment_stop)\n",
    "\n",
    "    save_pickle(comment_lemma, folder, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare basic pre-processing steps until before lemmatization\n",
    "\n",
    "# Train dataset\n",
    "train_clean = clean_data(bal_train_df)\n",
    "train_token = nltk_tokenize(train_clean)\n",
    "train_stopwords = nltk_stopwords(train_token)\n",
    "\n",
    "# Test dataset\n",
    "test_clean = clean_data(bal_test_df)\n",
    "test_token = nltk_tokenize(test_clean)\n",
    "test_stopwords = nltk_stopwords(test_clean)"
   ]
  },
  {
   "source": [
    "## Spacy Lemmatization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load small version of spacy's language model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "source": [
    "Since spacy tokenizes sentences automatically with their model, I have to replace their tokenizer with our used tokenizer (nltk) to ensure spacy's tokenizing function does not affect our result. I will need to modify nltk_tokenize() to return a spacy Doc object."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def nltk_tokenizer_for_spacy(text):\n",
    "    tokens = []\n",
    "\n",
    "    # your existing code to fill the list with tokens\n",
    "\n",
    "    # replace this line:\n",
    "    return tokens\n",
    "\n",
    "    # with this:\n",
    "    return Doc(nlp.vocab, tokens)"
   ]
  }
 ]
}