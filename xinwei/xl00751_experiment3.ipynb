{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0c8de7daaf6551741ca0923be7e98795069d9a1942406e7e8abaf0a7da2ca837d",
   "display_name": "Python 3.8.8 64-bit ('grp7_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Experiment 3\n",
    "## Comparing Text Featurization between Word2Vec and Doc2Vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Dropout, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "\n",
    "# Data Directory\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# Balanced datasets\n",
    "BALANCED_TRAIN_DATASET = \"data/balanced_dataset.pickle\"\n",
    "BALANCED_TEST_DATASET = \"data/balanced_test_dataset.pickle\"\n",
    "\n",
    "# Preprocessed balanced data\n",
    "PREPROCESSED_BAL_TRAIN_DATASET = \"data/preprocessed_train.pickle\"\n",
    "PREPROCESSED_BAL_TEST_DATASET = \"data/preprocessed_test.pickle\"\n",
    "\n",
    "# Word2Vec model\n",
    "WORD2VEC_MODEL = \"models/word2vec_model\"\n",
    "\n",
    "# Doc2Vec model\n",
    "DOC2VEC_MODEL = \"models/doc2vec_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save data as a .pickle file\n",
    "# Params: \n",
    "    # List or Dataframe - @data: Data to be saved as pickle\n",
    "    # Str - @folder: folder name\n",
    "    # Str - file name\n",
    "# Output: Pickle file in directory/repo \n",
    "def save_pickle(data, folder, file_name):\n",
    "    with open(\"{0}/{1}.pickle\".format(folder, file_name), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Saved data is stored in \\'{folder}\\' in the form of {file_name}.pickle\")\n",
    "    #pickle.dump(data, open(\"data/{0}.pickle\".format(file_name),\"wb\"))\n",
    "\n",
    "# Function to load pickle file\n",
    "# Params:\n",
    "    # Str - @file_path: File path of pickle file\n",
    "# Output:\n",
    "    # Saved object in original file type (list/dataframe)\n",
    "def load_pickle(file_path):\n",
    "    return pickle.load(open(file_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "# Get preprocessed train dataset\n",
    "bal_train_dataset = load_pickle(PREPROCESSED_BAL_TRAIN_DATASET)\n",
    "\n",
    "# Get preprocessed test dataset\n",
    "bal_test_dataset = load_pickle(PREPROCESSED_BAL_TEST_DATASET)\n",
    "\n",
    "# Get train_y\n",
    "bal_train_y = pd.read_pickle(BALANCED_TRAIN_DATASET)\n",
    "bal_train_y = bal_train_y.drop(columns=\"comment_text\")\n",
    "\n",
    "# Get test_y\n",
    "bal_test_y = pd.read_pickle(BALANCED_TEST_DATASET)\n",
    "bal_test_y = bal_test_y.drop(columns=\"comment_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment-specific imports\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "source": [
    "# Word2Vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec constants\n",
    "W2V_SIZE = 100          # default is 100\n",
    "W2V_WINDOW_SIZE = 5     # default is 5\n",
    "W2V_MIN_COUNT = 5       # default is 5\n",
    "W2V_SG = 0              # default is 0\n",
    "W2V_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and save Word2Vec model\n",
    "# Params: \n",
    "    # @sentences, @vector_size, @window, @min_count and @sg are gensim Word2Vec model params\n",
    "    # List      - @sentences:   tokens that have been fully pre-processed\n",
    "    # Int       - @size:        dimensionality of word vectors (typically between 100-300)\n",
    "    # Int       - @window_size: max distance between current and predicted word in a sentence\n",
    "    # Int       - @min_count:   ignores all words with total frequency lower than this\n",
    "    # Binary    - @sg:          training algorithm, 0 - CBOW, 1 - skip-gram \n",
    "    # Str       - @file_name:   model name\n",
    "# Output: Model file in directory/repo \n",
    "def word2vec_create_model(sentences, size, window, min_count, sg, file_name):\n",
    "    model = Word2Vec(sentences=sentences, size=size, window=window, min_count=min_count, sg=sg)\n",
    "    model.save(\"{0}.model\".format(file_name))\n",
    "\n",
    "# Function to load Word2Vec model\n",
    "# Params: Str - file name\n",
    "# Returns: Model - word2vec model\n",
    "def word2vec_load_model(file_name):\n",
    "    return Word2Vec.load(\"{0}.model\".format(file_name))"
   ]
  },
  {
   "source": [
    "The following cell creates the Word2Vec model that is trained once, then it can be saved and loaded without the need for running the model creation and training it again."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to save the model\n",
    "SAVE_WORD2VEC_MODEL = \"models/word2vec_model.model\"\n",
    "\n",
    "# Create Word2Vec CBOW Model\n",
    "word2vec_create_model(bal_train_dataset, W2V_SIZE, W2V_WINDOW_SIZE, W2V_MIN_COUNT, W2V_SG, WORD2VEC_MODEL)\n",
    "\n",
    "# Initialize model\n",
    "word2vec_model = word2vec_load_model(WORD2VEC_MODEL)\n",
    "\n",
    "# Train model\n",
    "word2vec_model.train(bal_train_dataset, total_examples=word2vec_model.corpus_count, epochs = 10)\n",
    "\n",
    "# Save trained model\n",
    "word2vec_model.save(SAVE_WORD2VEC_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "word2vec_model = word2vec_load_model(WORD2VEC_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['cocksucker', 'piss', 'around', 'work', 'gay', 'white', 'two', 'way', 'erase', 'comment', 'ww', 'holocaust', 'jew', 'head', 'go', 'meeting', 'doubt', 'word', 'bible', 'homosexuality', 'sin', 'make', 'forehead', 'mass', 'pal', 'first', 'last', 'warn', 'fuck', 'wont', 'appreciate', 'nazi', 'would', 'write', 'page', 'dont', 'wish', 'talk', 'anymore', 'dark', 'side', 'stupid', 'peace', 'shit', 'stop', 'delete', 'stuff', 'asshole', 'die', 'fall', 'hole', 'hell', 'hi', 'back', 'undo', 'edits', 'pair', 'weiner', 'think', 'fagget', 'get', 'burn', 'hate', 'sorry', 'cant', 'sex', 'im', 'run', 'reply', 'loser', 'un', 'defines', 'vietnam', 'part', 'southeast', 'asia', 'far', 'know', 'use', 'french', 'country', 'anyway', 'culture', 'always', 'influence', 'sea', 'han', 'chinese', 'proper', 'fringe', 'indigenous', 'tribe', 'admit', 'vietnamese', 'bunch', 'wannabe', 'crap', 'people', 'east', 'asian']\n[['cocksucker', 'piss', 'around', 'work'], ['gay', 'antisemmitian', 'archangel', 'white', 'tiger', 'meow', 'greetingshhh', 'uh', 'two', 'way', 'erase', 'comment', 'ww', 'holocaust', 'brutally', 'slay', 'jew', 'gaysgypsysslavsanyone', 'antisemitian', 'shave', 'head', 'bald', 'go', 'skinhead', 'meeting', 'doubt', 'word', 'bible', 'homosexuality', 'deadly', 'sin', 'make', 'pentagram', 'tatoo', 'forehead', 'go', 'satanistic', 'mass', 'gay', 'pal', 'first', 'last', 'warn', 'fuck', 'gay', 'wont', 'appreciate', 'nazi', 'shwain', 'would', 'write', 'page', 'dont', 'wish', 'talk', 'anymore', 'beware', 'dark', 'side']]\n"
     ]
    }
   ],
   "source": [
    "# Print vocab to test between vocab and dataset\n",
    "vocab = list(word2vec_model.wv.vocab)\n",
    "print(vocab[:100])\n",
    "print(bal_train_dataset[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of w1 'cocksucker': (100,)\nShape of w2 'piss': (100,)\nVocab: 4679\nLength of the vector for w3 'white': 100\n"
     ]
    }
   ],
   "source": [
    "# Printing vectors and vocab \n",
    "w1 = bal_train_dataset[0][0] # 1st word of 1st document\n",
    "w2 = bal_train_dataset[0][1] # 2nd word of 1st document\n",
    "print(f\"Shape of w1 \\'{w1}\\': {word2vec_model.wv.get_vector(w1).shape}\")\n",
    "print(f\"Shape of w2 \\'{w2}\\': {word2vec_model.wv.get_vector(w2).shape}\")\n",
    " \n",
    "print(\"Vocab:\", len(word2vec_model.wv.vocab))\n",
    "\n",
    "# Print the size of the word2vec vector for one word\n",
    "w3 = bal_train_dataset[1][3]\n",
    "print(f\"Length of the vector for w3 \\'{w3}\\':\", len(word2vec_model.wv.get_vector(w3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words not in word2vec model\n",
    "# Params: \n",
    "#   Word2Vec Model  - @model:           Word2Vec Model\n",
    "#   List            - @all_comments:    Pre-processed tokens (2D List)\n",
    "# Output: List - Tokens with only words in model's vocab (2D List)\n",
    "def word2vec_remove_words_outside_vocab(model, all_comments):\n",
    "    # Remove words not in w2v cbow model vocab\n",
    "    doc = []\n",
    "    for comment in all_comments:\n",
    "        temp = []\n",
    "        for word in comment:\n",
    "            if word in model.wv.vocab:\n",
    "                temp.append(word)\n",
    "        doc.append(temp)\n",
    "    return doc\n",
    "\n",
    "# Average word vectors of each comment\n",
    "# Params:\n",
    "#   Word2Vec Model  - @model:             Word2Vec Model \n",
    "#   List            - @comment_vocab:     Tokens with only words in model's vocab (2D List)\n",
    "# Output: \n",
    "#   Numpy Array of average vector of each comment\n",
    "def word2vec_average_vectors(model, comment_vocab):\n",
    "\n",
    "    average_list = []\n",
    "\n",
    "    for comment in comment_vocab:\n",
    "\n",
    "        comment_vectors = []\n",
    "\n",
    "        for word in comment:\n",
    "            comment_vectors.append((model.wv.get_vector(word)))\n",
    "        \n",
    "        comment_vectors_np = np.asarray(comment_vectors, dtype='float32')\n",
    "        mean = np.mean(comment_vectors_np, keepdims=True)\n",
    "        average_list.append(mean)\n",
    "    \n",
    "    return np.asarray(average_list, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['cocksucker', 'piss', 'around', 'work'], ['gay', 'white', 'two', 'way', 'erase', 'comment', 'ww', 'holocaust', 'jew', 'head', 'go', 'meeting', 'doubt', 'word', 'bible', 'homosexuality', 'sin', 'make', 'forehead', 'go', 'mass', 'gay', 'pal', 'first', 'last', 'warn', 'fuck', 'gay', 'wont', 'appreciate', 'nazi', 'would', 'write', 'page', 'dont', 'wish', 'talk', 'anymore', 'dark', 'side']]\n[['cocksucker', 'piss', 'around', 'work'], ['gay', 'antisemmitian', 'archangel', 'white', 'tiger', 'meow', 'greetingshhh', 'uh', 'two', 'way', 'erase', 'comment', 'ww', 'holocaust', 'brutally', 'slay', 'jew', 'gaysgypsysslavsanyone', 'antisemitian', 'shave', 'head', 'bald', 'go', 'skinhead', 'meeting', 'doubt', 'word', 'bible', 'homosexuality', 'deadly', 'sin', 'make', 'pentagram', 'tatoo', 'forehead', 'go', 'satanistic', 'mass', 'gay', 'pal', 'first', 'last', 'warn', 'fuck', 'gay', 'wont', 'appreciate', 'nazi', 'shwain', 'would', 'write', 'page', 'dont', 'wish', 'talk', 'anymore', 'beware', 'dark', 'side']]\n"
     ]
    }
   ],
   "source": [
    "# Remove words that are not in Word2Vec vocab\n",
    "word2vec_removed_words = word2vec_remove_words_outside_vocab(word2vec_model, bal_train_dataset)\n",
    "\n",
    "# Print word2vec_removed_words to compare against original text\n",
    "print(word2vec_removed_words[:2])\n",
    "print(bal_train_dataset[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\lamxw\\anaconda3\\envs\\grp7_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\nC:\\Users\\lamxw\\anaconda3\\envs\\grp7_env\\lib\\site-packages\\numpy\\core\\_methods.py:162: RuntimeWarning: invalid value encountered in true_divide\n  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average of each comment\n",
    "word2vec_average = word2vec_average_vectors(word2vec_model, word2vec_removed_words)\n",
    "# Warning appears because some documents are empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for keras model\n",
    "NUM_WORDS = 20000\n",
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad vectors as needed that cannot be used with Keras' sequential padding\n",
    "# Params: \n",
    "    # Numpy array - @vector_array: Vectors of data to be used in keras converted to np array\n",
    "# Outputs: \n",
    "    # Numpy array - @padded_array: Padded vectors\n",
    "def pad(vector_array, max_len):\n",
    "    padded_array = np.zeros((max_len, vector_array.shape[-1]))\n",
    "    padded_array[:len(vector_array),:] = vector_array\n",
    "    return padded_array\n",
    "\n",
    "# Function taken from utils, slightly modified\n",
    "def modified_build_model():\n",
    "    EPOCHS = 30\n",
    "    INIT_LR = 1e-3\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #model.add(Embedding(num_words, 128))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3))\n",
    "    model.add(Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "    adam = tf.keras.optimizers.Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer=adam,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad word2vec_average to use for training\n",
    "word2vec_train_x = np.stack(list(map(lambda x: pad(x, MAX_LEN), word2vec_average)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "119/119 [==============================] - 34s 31ms/step - loss: 0.6338 - accuracy: 0.4326\n",
      "Epoch 2/30\n",
      "119/119 [==============================] - 3s 28ms/step - loss: 0.5588 - accuracy: 0.3177\n",
      "Epoch 3/30\n",
      "119/119 [==============================] - 3s 21ms/step - loss: 0.5515 - accuracy: 0.3360\n",
      "Epoch 4/30\n",
      "119/119 [==============================] - 2s 16ms/step - loss: 0.5559 - accuracy: 0.3271\n",
      "Epoch 5/30\n",
      "119/119 [==============================] - 2s 16ms/step - loss: 0.5554 - accuracy: 0.3214\n",
      "Epoch 6/30\n",
      "119/119 [==============================] - 2s 15ms/step - loss: 0.5529 - accuracy: 0.2910\n",
      "Epoch 7/30\n",
      "119/119 [==============================] - 2s 16ms/step - loss: 0.5508 - accuracy: 0.3084\n",
      "Epoch 8/30\n",
      "119/119 [==============================] - 3s 23ms/step - loss: 0.5538 - accuracy: 0.3421\n",
      "Epoch 9/30\n",
      "119/119 [==============================] - 2s 17ms/step - loss: 0.5496 - accuracy: 0.3530\n",
      "Epoch 10/30\n",
      "119/119 [==============================] - 2s 17ms/step - loss: 0.5564 - accuracy: 0.3069\n",
      "Epoch 11/30\n",
      "119/119 [==============================] - 2s 21ms/step - loss: 0.5498 - accuracy: 0.2410\n",
      "Epoch 12/30\n",
      "119/119 [==============================] - 3s 27ms/step - loss: 0.5508 - accuracy: 0.2819\n",
      "Epoch 13/30\n",
      "119/119 [==============================] - 3s 23ms/step - loss: 0.5546 - accuracy: 0.3227\n",
      "Epoch 14/30\n",
      "119/119 [==============================] - 2s 17ms/step - loss: 0.5450 - accuracy: 0.2789\n",
      "Epoch 15/30\n",
      "119/119 [==============================] - 2s 17ms/step - loss: 0.5474 - accuracy: 0.3373\n",
      "Epoch 16/30\n",
      "119/119 [==============================] - 2s 17ms/step - loss: 0.5526 - accuracy: 0.2879\n",
      "Epoch 17/30\n",
      "119/119 [==============================] - 2s 16ms/step - loss: 0.5501 - accuracy: 0.2467\n",
      "Epoch 18/30\n",
      "119/119 [==============================] - 2s 16ms/step - loss: 0.5550 - accuracy: 0.2742\n",
      "Epoch 19/30\n",
      "119/119 [==============================] - 2s 18ms/step - loss: 0.5549 - accuracy: 0.3291\n",
      "Epoch 20/30\n",
      "119/119 [==============================] - 2s 20ms/step - loss: 0.5517 - accuracy: 0.2114\n",
      "Epoch 21/30\n",
      "119/119 [==============================] - 2s 17ms/step - loss: 0.5533 - accuracy: 0.2467\n",
      "Epoch 22/30\n",
      "119/119 [==============================] - 2s 16ms/step - loss: 0.5460 - accuracy: 0.1647\n",
      "Epoch 23/30\n",
      "119/119 [==============================] - 2s 17ms/step - loss: 0.5492 - accuracy: 0.2605\n",
      "Epoch 24/30\n",
      "119/119 [==============================] - 2s 16ms/step - loss: 0.5507 - accuracy: 0.2495\n",
      "Epoch 25/30\n",
      "119/119 [==============================] - 2s 16ms/step - loss: 0.5476 - accuracy: 0.1748\n",
      "Epoch 26/30\n",
      "119/119 [==============================] - 2s 16ms/step - loss: 0.5508 - accuracy: 0.3226\n",
      "Epoch 27/30\n",
      "119/119 [==============================] - 2s 16ms/step - loss: 0.5524 - accuracy: 0.1833\n",
      "Epoch 28/30\n",
      "119/119 [==============================] - 2s 16ms/step - loss: 0.5502 - accuracy: 0.2230\n",
      "Epoch 29/30\n",
      "119/119 [==============================] - 2s 16ms/step - loss: 0.5510 - accuracy: 0.2314\n",
      "Epoch 30/30\n",
      "119/119 [==============================] - 2s 16ms/step - loss: 0.5584 - accuracy: 0.2083\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c37dc01760>"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# Train model\n",
    "model_w2v = modified_build_model()\n",
    "\n",
    "model_w2v.fit(word2vec_train_x, bal_train_y, batch_size=60, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\lamxw\\anaconda3\\envs\\grp7_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\nC:\\Users\\lamxw\\anaconda3\\envs\\grp7_env\\lib\\site-packages\\numpy\\core\\_methods.py:162: RuntimeWarning: invalid value encountered in true_divide\n  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "# Prepare test dataset\n",
    "\n",
    "# Remove words that are not in Word2Vec vocab\n",
    "word2vec_remove_test = word2vec_remove_words_outside_vocab(word2vec_model, bal_test_dataset)\n",
    "\n",
    "# Calculate the average of each comment\n",
    "word2vec_average_test = word2vec_average_vectors(word2vec_model, word2vec_remove_test)\n",
    "\n",
    "# Pad word2vec_average to use for training\n",
    "word2vec_test_x = np.stack(list(map(lambda x: pad(x, MAX_LEN), word2vec_average_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "117/117 [==============================] - 2s 11ms/step - loss: 0.4597 - accuracy: 0.0093\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.4596764147281647, 0.009314989671111107]"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# Evaluate model for Doc2Vec\n",
    "model_w2v.evaluate(word2vec_test_x, bal_test_y, batch_size=60)"
   ]
  },
  {
   "source": [
    "# Doc2Vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim Doc2Vec constants\n",
    "D2V_SIZE = 100          # default is 100\n",
    "D2V_WINDOW_SIZE = 5     # default is 5\n",
    "D2V_MIN_COUNT = 5       # default is 5\n",
    "D2V_DM_MEAN = 1\n",
    "D2V_DBOW_WORDS = 0\n",
    "D2V_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and save Doc2Vec model\n",
    "# Params: \n",
    "    # @documents, @vector_size, @window, @min_count, @dm_mean and @dbow_words are Doc2Vec model params\n",
    "    # TaggedDocument    - @documents:   input corpus with pre-processed tokens in 2D list form\n",
    "    # Int               - @size:        dimensionality of word vectors (typically between 100-300)\n",
    "    # Int               - @window: max distance between current and predicted word in a sentence\n",
    "    # Int               - @min_count:   ignores all words with total frequency lower than this\n",
    "    # Binary            - @dm_mean:     sum or mean of word vectors; 0 - sum, 1 - mean\n",
    "    # Binary            - @dbow_words:  training algorithm, 0 - bow, 1 - skip-gram and bow \n",
    "    # Str               - @file_name:   model name\n",
    "# Output: Model file in directory/repo \n",
    "def doc2vec_create_model(documents, size, window, min_count, dm_mean, dbow_words, file_name):\n",
    "    model = Doc2Vec(documents=documents, vector_size=size, window=window, min_count=min_count, dm_mean=dm_mean, dbow_words=dbow_words)\n",
    "    model.save(\"{0}.model\".format(file_name))\n",
    "\n",
    "# Function to load Doc2Vec model\n",
    "# Params: Str - file name\n",
    "# Returns: Model - doc2vec model\n",
    "def doc2vec_load_model(file_name):\n",
    "    return Word2Vec.load(\"{0}.model\".format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up comments for Gensim Doc2Vec Model\n",
    "comments = [TaggedDocument(comment, [i]) for i, comment in enumerate(bal_train_dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to save the model\n",
    "SAVE_DOC2VEC_MODEL = \"models/doc2vec_model.model\"\n",
    "\n",
    "# Create Doc2Vec model\n",
    "doc2vec_create_model(comments, D2V_SIZE, D2V_WINDOW_SIZE, D2V_MIN_COUNT, D2V_DM_MEAN, D2V_DBOW_WORDS, DOC2VEC_MODEL)\n",
    "\n",
    "# Initialize model\n",
    "doc2vec_model = doc2vec_load_model(DOC2VEC_MODEL)\n",
    "\n",
    "# Train model\n",
    "doc2vec_model.train(comments, total_examples=doc2vec_model.corpus_count, epochs=D2V_EPOCHS)\n",
    "\n",
    "# Save trained model\n",
    "doc2vec_model.save(SAVE_DOC2VEC_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "doc2vec_model = doc2vec_load_model(DOC2VEC_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of w1 'cocksucker': (100,)\nShape of w2 'piss': (100,)\nVocab: 4679\nLength of the vector for w3 'white': 100\nVector of doc 0: (100,)\n"
     ]
    }
   ],
   "source": [
    "# Printing vectors and vocab\n",
    "w1 = bal_train_dataset[0][0] # 1st word of 1st document\n",
    "w2 = bal_train_dataset[0][1] # 2nd word of 1st document\n",
    "print(f\"Shape of w1 \\'{w1}\\': {doc2vec_model.wv.get_vector(w1).shape}\")\n",
    "print(f\"Shape of w2 \\'{w2}\\': {doc2vec_model.wv.get_vector(w2).shape}\")\n",
    "\n",
    "print(\"Vocab:\", len(doc2vec_model.wv.vocab))\n",
    "\n",
    "# Print the size of the word2vec vector for one word\n",
    "w3 = bal_train_dataset[1][3]\n",
    "print(f\"Length of the vector for w3 \\'{w3}\\':\", len(doc2vec_model.wv.get_vector(w3)))\n",
    "\n",
    "# Print the vector of 1st document\n",
    "print(\"Vector of doc 0:\", doc2vec_model.docvecs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({0: 5145, 1: 367, 2: 196, 3: 118, 4: 60, 5: 56, 6: 46, 8: 43, 7: 35, 10: 29, 13: 26, 9: 23, 12: 22, 17: 21, 11: 19, 19: 14, 15: 14, 14: 14, 16: 13, 22: 13, 20: 12, 27: 12, 26: 12, 18: 11, 35: 11, 24: 11, 32: 11, 25: 10, 21: 10, 29: 10, 28: 10, 39: 10, 38: 9, 31: 8, 34: 7, 37: 7, 23: 7, 30: 7, 47: 7, 58: 6, 40: 6, 43: 5, 51: 5, 33: 5, 45: 5, 64: 5, 91: 5, 46: 5, 41: 5, 70: 5, 87: 4, 48: 4, 80: 4, 263: 4, 79: 4, 94: 4, 71: 4, 110: 4, 42: 4, 95: 4, 115: 4, 53: 4, 81: 3, 36: 3, 430: 3, 57: 3, 107: 3, 100: 3, 84: 3, 44: 3, 63: 3, 66: 3, 92: 3, 61: 3, 55: 3, 86: 3, 105: 3, 102: 3, 54: 3, 111: 3, 83: 3, 56: 3, 50: 3, 60: 3, 49: 3, 293: 3, 284: 2, 199: 2, 132: 2, 471: 2, 933: 2, 2901: 2, 2121: 2, 198: 2, 203: 2, 231: 2, 74: 2, 357: 2, 127: 2, 90: 2, 652: 2, 185: 2, 178: 2, 176: 2, 98: 2, 138: 2, 162: 2, 108: 2, 302: 2, 853: 2, 89: 2, 422: 2, 301: 2, 160: 2, 67: 2, 171: 2, 950: 2, 112: 2, 135: 2, 282: 2, 191: 2, 633: 2, 147: 2, 144: 2, 211: 2, 131: 2, 62: 2, 113: 2, 137: 2, 303: 2, 272: 2, 159: 2, 52: 2, 158: 2, 72: 2, 151: 2, 120: 2, 97: 2, 215: 2, 77: 2, 85: 2, 236: 2, 68: 2, 126: 2, 222: 1, 4589: 1, 2311: 1, 139: 1, 2793: 1, 1507: 1, 988: 1, 3485: 1, 5829: 1, 1561: 1, 3777: 1, 1296: 1, 786: 1, 59: 1, 5517: 1, 1461: 1, 1182: 1, 341: 1, 1346: 1, 2851: 1, 404: 1, 3601: 1, 304: 1, 760: 1, 1817: 1, 234: 1, 4348: 1, 142: 1, 2927: 1, 553: 1, 828: 1, 4101: 1, 574: 1, 1663: 1, 2964: 1, 467: 1, 535: 1, 123: 1, 2284: 1, 173: 1, 494: 1, 4995: 1, 461: 1, 3813: 1, 718: 1, 1352: 1, 240: 1, 327: 1, 209: 1, 379: 1, 1063: 1, 204: 1, 4700: 1, 4459: 1, 347: 1, 4567: 1, 3163: 1, 387: 1, 783: 1, 2567: 1, 3079: 1, 4961: 1, 2017: 1, 423: 1, 5346: 1, 564: 1, 1018: 1, 2978: 1, 118: 1, 2328: 1, 565: 1, 3749: 1, 4186: 1, 1593: 1, 508: 1, 4609: 1, 2013: 1, 1518: 1, 149: 1, 3386: 1, 6498: 1, 1389: 1, 1109: 1, 602: 1, 1933: 1, 1469: 1, 3758: 1, 181: 1, 736: 1, 434: 1, 3625: 1, 103: 1, 2362: 1, 117: 1, 669: 1, 153: 1, 407: 1, 6131: 1, 3637: 1, 157: 1, 826: 1, 1516: 1, 268: 1, 202: 1, 145: 1, 1559: 1, 182: 1, 264: 1, 1622: 1, 2375: 1, 3877: 1, 2222: 1, 1029: 1, 134: 1, 2465: 1, 637: 1, 861: 1, 235: 1, 3054: 1, 2749: 1, 1504: 1, 977: 1, 373: 1, 1368: 1, 73: 1, 5817: 1, 143: 1, 6971: 1, 484: 1, 6486: 1, 3707: 1, 496: 1, 510: 1, 614: 1, 3332: 1, 2206: 1, 228: 1, 2095: 1, 154: 1, 3710: 1, 109: 1, 2576: 1, 446: 1, 1601: 1, 1107: 1, 411: 1, 5206: 1, 4990: 1, 4908: 1, 116: 1, 551: 1, 164: 1, 237: 1, 99: 1, 2772: 1, 687: 1, 322: 1, 2973: 1, 689: 1, 625: 1, 5501: 1, 255: 1, 4922: 1, 1317: 1, 953: 1, 175: 1, 177: 1, 428: 1, 474: 1, 229: 1, 3975: 1, 1046: 1, 2037: 1, 549: 1, 346: 1, 1764: 1, 972: 1, 400: 1, 1726: 1, 1085: 1, 413: 1, 3430: 1, 563: 1, 944: 1, 7103: 1, 868: 1, 1048: 1, 1142: 1, 2666: 1, 1121: 1, 161: 1, 6244: 1, 903: 1, 1113: 1, 150: 1, 2505: 1, 323: 1, 76: 1, 1443: 1, 3969: 1, 1193: 1, 728: 1, 533: 1, 1041: 1, 973: 1, 825: 1, 5458: 1, 2741: 1, 473: 1, 7015: 1, 1500: 1, 277: 1, 573: 1, 595: 1, 3562: 1, 934: 1, 297: 1, 65: 1, 5197: 1, 1993: 1, 521: 1, 663: 1, 4736: 1, 3342: 1, 529: 1, 1294: 1, 2516: 1, 4278: 1, 6071: 1, 2432: 1, 214: 1, 2055: 1, 457: 1, 440: 1, 796: 1, 5106: 1, 3716: 1, 324: 1, 3444: 1, 5929: 1, 2527: 1, 163: 1, 2346: 1, 363: 1, 1077: 1, 1025: 1, 1042: 1, 2800: 1, 660: 1, 371: 1, 567: 1, 2658: 1, 2141: 1, 292: 1, 1867: 1, 369: 1, 385: 1, 1071: 1, 225: 1, 5522: 1, 122: 1, 253: 1, 1672: 1, 4848: 1, 3713: 1, 6075: 1, 312: 1, 409: 1, 635: 1, 180: 1, 106: 1, 376: 1, 2562: 1, 447: 1, 503: 1, 1772: 1, 148: 1, 1298: 1, 394: 1, 1400: 1, 242: 1, 504: 1, 196: 1, 1160: 1, 1117: 1, 5737: 1, 2844: 1, 186: 1, 273: 1, 6364: 1, 2298: 1, 1954: 1, 3560: 1, 174: 1, 491: 1, 569: 1, 3771: 1, 611: 1, 250: 1, 2488: 1, 1589: 1, 232: 1, 133: 1, 325: 1, 1203: 1, 1367: 1, 6245: 1, 1184: 1, 319: 1, 188: 1, 2099: 1, 1551: 1, 399: 1, 1315: 1, 947: 1, 4759: 1, 5922: 1, 130: 1, 761: 1, 121: 1, 351: 1, 243: 1, 485: 1, 1092: 1, 886: 1, 223: 1, 555: 1, 1155: 1, 1148: 1, 772: 1, 217: 1, 412: 1, 349: 1, 4135: 1, 278: 1, 288: 1, 458: 1, 252: 1, 391: 1, 187: 1, 1985: 1, 308: 1, 192: 1, 1167: 1, 784: 1, 465: 1, 1035: 1, 3077: 1, 1153: 1, 403: 1, 7005: 1, 1110: 1, 6776: 1, 460: 1, 1275: 1, 213: 1, 1198: 1, 2759: 1, 168: 1, 709: 1, 1404: 1, 2578: 1, 241: 1, 793: 1, 1293: 1, 482: 1, 2231: 1, 2752: 1, 2957: 1, 1606: 1, 1033: 1, 360: 1, 4793: 1, 814: 1, 2781: 1, 651: 1, 4564: 1, 686: 1, 3957: 1, 577: 1, 540: 1, 855: 1, 69: 1})\n"
     ]
    }
   ],
   "source": [
    "# Assessing the model with Gensim Doc2Vec tutorial\n",
    "\n",
    "# Infer new vectors for each document of the training corpus, compare them against the actual vectors, then return the rank of the document based on self-similarity\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "\n",
    "for doc_id in range(len(comments)):\n",
    "    inferred_vector = doc2vec_model.infer_vector(comments[doc_id].words)\n",
    "    sims = doc2vec_model.docvecs.most_similar([inferred_vector], topn=len(doc2vec_model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "\n",
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Document (7131): «really dont think understand come idea bad right away kind community go bad idea go away instead help rewrite»\n\nSIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d100,n5,w5,mc5,s0.001,t3):\n\nMOST (185, 0.785494327545166): «people pretty overzealous whole free thing get fuck life fuck nigger»\n\nSECOND-MOST (6424, 0.7661423683166504): «bad behaviour»\n\nMEDIAN (5526, 0.3718322515487671): «image requestedpeople kansasamerican football people»\n\nLEAST (899, -0.7124570608139038): «fuck u weak as niggaz»\n\n"
     ]
    }
   ],
   "source": [
    "# Assessing the model with Gensim Doc2Vec tutorial\n",
    "\n",
    "# Test most & least similar documents against random document\n",
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(comments[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % doc2vec_model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(comments[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data for model\n",
    "# Convert doc2vec vectors to numpy array for Keras to use\n",
    "d2v_train_x = np.array([doc2vec_model.docvecs[i] for i, comment in enumerate(comments)])\n",
    "# Pad train_x\n",
    "d2v_padded_x = np.stack(list(map(lambda x: pad(x, MAX_LEN), d2v_train_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "119/119 [==============================] - 5s 24ms/step - loss: 0.5409 - accuracy: 0.2676\n",
      "Epoch 2/30\n",
      "119/119 [==============================] - 4s 30ms/step - loss: 0.4216 - accuracy: 0.2953\n",
      "Epoch 3/30\n",
      "119/119 [==============================] - 3s 26ms/step - loss: 0.3911 - accuracy: 0.3267\n",
      "Epoch 4/30\n",
      "119/119 [==============================] - 2s 18ms/step - loss: 0.3825 - accuracy: 0.3462\n",
      "Epoch 5/30\n",
      "119/119 [==============================] - 2s 17ms/step - loss: 0.3635 - accuracy: 0.3578\n",
      "Epoch 6/30\n",
      "119/119 [==============================] - 2s 18ms/step - loss: 0.3683 - accuracy: 0.3424\n",
      "Epoch 7/30\n",
      "119/119 [==============================] - 2s 21ms/step - loss: 0.3547 - accuracy: 0.3661\n",
      "Epoch 8/30\n",
      "119/119 [==============================] - 3s 22ms/step - loss: 0.3353 - accuracy: 0.3882\n",
      "Epoch 9/30\n",
      "119/119 [==============================] - 3s 29ms/step - loss: 0.3369 - accuracy: 0.3755\n",
      "Epoch 10/30\n",
      "119/119 [==============================] - 3s 29ms/step - loss: 0.3274 - accuracy: 0.3830\n",
      "Epoch 11/30\n",
      "119/119 [==============================] - 2s 18ms/step - loss: 0.3258 - accuracy: 0.3730\n",
      "Epoch 12/30\n",
      "119/119 [==============================] - 2s 18ms/step - loss: 0.3219 - accuracy: 0.3683\n",
      "Epoch 13/30\n",
      "119/119 [==============================] - 2s 18ms/step - loss: 0.3058 - accuracy: 0.3622\n",
      "Epoch 14/30\n",
      "119/119 [==============================] - 2s 17ms/step - loss: 0.3043 - accuracy: 0.3673\n",
      "Epoch 15/30\n",
      "119/119 [==============================] - 2s 21ms/step - loss: 0.3039 - accuracy: 0.3337\n",
      "Epoch 16/30\n",
      "119/119 [==============================] - 4s 31ms/step - loss: 0.2941 - accuracy: 0.3577\n",
      "Epoch 17/30\n",
      "119/119 [==============================] - 4s 30ms/step - loss: 0.2978 - accuracy: 0.3551\n",
      "Epoch 18/30\n",
      "119/119 [==============================] - 4s 31ms/step - loss: 0.2883 - accuracy: 0.3487\n",
      "Epoch 19/30\n",
      "119/119 [==============================] - 4s 30ms/step - loss: 0.2757 - accuracy: 0.3394\n",
      "Epoch 20/30\n",
      "119/119 [==============================] - 4s 29ms/step - loss: 0.2701 - accuracy: 0.3592\n",
      "Epoch 21/30\n",
      "119/119 [==============================] - 3s 28ms/step - loss: 0.2748 - accuracy: 0.3568\n",
      "Epoch 22/30\n",
      "119/119 [==============================] - 2s 21ms/step - loss: 0.2616 - accuracy: 0.3572\n",
      "Epoch 23/30\n",
      "119/119 [==============================] - 5s 39ms/step - loss: 0.2605 - accuracy: 0.3637\n",
      "Epoch 24/30\n",
      "119/119 [==============================] - 4s 33ms/step - loss: 0.2551 - accuracy: 0.3812\n",
      "Epoch 25/30\n",
      "119/119 [==============================] - 5s 38ms/step - loss: 0.2560 - accuracy: 0.3697\n",
      "Epoch 26/30\n",
      "119/119 [==============================] - 4s 37ms/step - loss: 0.2463 - accuracy: 0.3641\n",
      "Epoch 27/30\n",
      "119/119 [==============================] - 5s 39ms/step - loss: 0.2454 - accuracy: 0.3842\n",
      "Epoch 28/30\n",
      "119/119 [==============================] - 5s 39ms/step - loss: 0.2340 - accuracy: 0.3402\n",
      "Epoch 29/30\n",
      "119/119 [==============================] - 5s 39ms/step - loss: 0.2370 - accuracy: 0.3883\n",
      "Epoch 30/30\n",
      "119/119 [==============================] - 5s 38ms/step - loss: 0.2239 - accuracy: 0.3609\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c37e4abdc0>"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "# Fit model for Doc2Vec\n",
    "model_d2v = modified_build_model()\n",
    "\n",
    "model_d2v.fit(d2v_padded_x, bal_train_y, batch_size=60, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer vectors for test_x based on Doc2Vec model\n",
    "def prepare_vectors(test_x):\n",
    "    \n",
    "    vector_x = [doc2vec_model.infer_vector(comment) for comment in test_x]\n",
    "\n",
    "    return vector_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test_x for evaluation and prediction\n",
    "d2v_vector_x = prepare_vectors(bal_test_dataset)\n",
    "\n",
    "# Convert doc2vec vectors to numpy array for Keras to use\n",
    "d2v_np_x = np.array(d2v_vector_x)\n",
    "\n",
    "# Pad test_x\n",
    "d2v_test_x = np.stack(list(map(lambda x: pad(x, max_len=100), d2v_np_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "117/117 [==============================] - 3s 13ms/step - loss: 0.4755 - accuracy: 0.3019\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.47548913955688477, 0.30194899439811707]"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "# Evaluate model for Doc2Vec\n",
    "model_d2v.evaluate(d2v_test_x, bal_test_y, batch_size=60)"
   ]
  }
 ]
}