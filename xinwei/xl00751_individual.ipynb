{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0c8de7daaf6551741ca0923be7e98795069d9a1942406e7e8abaf0a7da2ca837d",
   "display_name": "Python 3.8.8 64-bit ('grp7_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec constants\n",
    "W2V_SIZE = 100          # default is 100\n",
    "W2V_WINDOW_SIZE = 5     # default is 5\n",
    "W2V_MIN_COUNT = 5       # default is 5\n",
    "W2V_SG = 0              # default is 0\n",
    "W2V_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lemmatized comments\n",
    "comment_lemma = pickle.load(open(\"comment_lemma.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and save Word2Vec model\n",
    "# Params: \n",
    "    # @sentences, @vector_size, @window, @min_count and @sg are gensim Word2Vec model params\n",
    "    # List      - @sentences:   tokens that have been fully pre-processed\n",
    "    # Int       - @size:        dimensionality of word vectors (typically between 100-300)\n",
    "    # Int       - @window_size: max distance between current and predicted word in a sentence\n",
    "    # Int       - @min_count:   ignores all words with total frequency lower than this\n",
    "    # Binary    - @sg:          training algorithm, 0 - CBOW, 1 - skip-gram \n",
    "    # Str       - @file_name:   model name\n",
    "# Output: Model file in directory/repo \n",
    "def word2vec_create_model(sentences, size, window, min_count, sg, file_name):\n",
    "    model = Word2Vec(sentences=sentences, size=size, window=window, min_count=min_count, sg=sg)\n",
    "    model.save(\"{0}.model\".format(file_name))\n",
    "\n",
    "# Function to load Word2Vec model\n",
    "# Params: Str - file name\n",
    "# Returns: Model - word2vec model\n",
    "def word2vec_load_model(file_name):\n",
    "    return Word2Vec.load(\"{0}.model\".format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Word2Vec CBOW Model\n",
    "word2vec_create_model(comment_lemma, W2V_SIZE, W2V_WINDOW_SIZE, W2V_MIN_COUNT, W2V_SG, \"word2vec_cbow_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and build vocab\n",
    "word2vec_cbow_model = word2vec_load_model(\"word2vec_cbow_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3127332, 4703120)"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "sentences_count = len(comment_lemma)\n",
    "# Train model\n",
    "word2vec_cbow_model.train(comment_lemma, total_examples=sentences_count, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['cocksucker', 'before', 'you', 'piss', 'around', 'on', 'my', 'work', 'be', 'gay', 'or', 'white', 'there', 'two', 'way', 'why', 'do', 'erase', 'comment', 'about', 'ww', 'that', 'holocaust', 'of', 'jew', 'and', 'not', 'if', 'than', 'your', 'head', 'go', 'to', 'the', 'meeting', 'doubt', 'word', 'bible', 'homosexuality', 'a', 'sin', 'make', 'forehead', 'mass', 'with', 'pal', 'first', 'last', 'warn', 'fuck', 'i', 'wont', 'appreciate', 'any', 'more', 'nazi', 'would', 'write', 'in', 'page', 'dont', 'wish', 'talk', 'anymore', 'dark', 'side', 'stupid', 'peace', 'shit', 'stop', 'delete', 'stuff', 'asshole', 'die', 'fall', 'hole', 'hell', 'hi', 'back', 'again', 'undo', 'edits', 'pair', 'weiner', 'think', 'fagget', 'get', 'burn', 'hate', 'm', 'sorry', 'we', 'cant', 'have', 'sex', 'im', 'run', 'out', 'reply', 'above']\n[['cocksucker', 'before', 'you', 'piss', 'around', 'on', 'my', 'work'], ['you', 'be', 'gay', 'or', 'antisemmitian', 'archangel', 'white', 'tiger', 'meow', 'greetingshhh', 'uh', 'there', 'be', 'two', 'way', 'why', 'you', 'do', 'erase', 'my', 'comment', 'about', 'ww', 'that', 'holocaust', 'be', 'brutally', 'slay', 'of', 'jew', 'and', 'not', 'gaysgypsysslavsanyone', 'if', 'you', 'be', 'antisemitian', 'than', 'shave', 'your', 'head', 'bald', 'and', 'go', 'to', 'the', 'skinhead', 'meeting', 'if', 'you', 'doubt', 'word', 'of', 'the', 'bible', 'that', 'homosexuality', 'be', 'a', 'deadly', 'sin', 'make', 'a', 'pentagram', 'tatoo', 'on', 'your', 'forehead', 'go', 'to', 'the', 'satanistic', 'mass', 'with', 'your', 'gay', 'pal', 'first', 'and', 'last', 'warn', 'you', 'fuck', 'gay', 'i', 'wont', 'appreciate', 'if', 'any', 'more', 'nazi', 'shwain', 'would', 'write', 'in', 'my', 'page', 'i', 'dont', 'wish', 'to', 'talk', 'to', 'you', 'anymore', 'beware', 'of', 'the', 'dark', 'side']]\n"
     ]
    }
   ],
   "source": [
    "# Print vocab\n",
    "vocab = list(word2vec_cbow_model.wv.vocab)\n",
    "print(vocab[:100])\n",
    "print(comment_lemma[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(100,)\nVocab: {0} 4784\nLength of the vector generated for a word\n100\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "w1 = \"cocksucker\"\n",
    "w2 = \"piss\"\n",
    "print(word2vec_cbow_model.wv.get_vector(w1).shape)\n",
    "# Vocab \n",
    "print(\"Vocab: {0}\", len(word2vec_cbow_model.wv.vocab))\n",
    "# Print the size of the word2vec vector for one word\n",
    "print(\"Length of the vector generated for a word\")\n",
    "print(len(word2vec_cbow_model.wv.get_vector('you')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words not in word2vec model\n",
    "# Params: \n",
    "#   Word2Vec Model  - @model:           Word2Vec Model\n",
    "#   List            - @all_comments:    Pre-processed tokens (2D List)\n",
    "# Output: List - Tokens with only words in model's vocab (2D List)\n",
    "def word2vec_remove_words_outside_vocab(model, all_comments):\n",
    "    # Remove words not in w2v cbow model vocab\n",
    "    doc = []\n",
    "    for comment in all_comments:\n",
    "        temp = []\n",
    "        for word in comment:\n",
    "            if word in model.wv.vocab:\n",
    "                temp.append(word)\n",
    "        doc.append(temp)\n",
    "    return doc\n",
    "\n",
    "# Average word vectors of each comment\n",
    "# Params:\n",
    "#   Word2Vec Model  -   @model:             Word2Vec Model \n",
    "#   List -              @comment_vocab:     Tokens with only words in model's vocab (2D List)\n",
    "# Output: \n",
    "def word2vec_average_vectors(model, comment_vocab):\n",
    "    return np.mean(model.wv.get_vector, axis=1)\n",
    "    #doc = [word for comment in all_comments for word in comment if word in gensim_cbow_model.wv.vocab]\n",
    "    #print(doc[:10])\n",
    "    #return np.mean(word2vec_model[doc], axis=0)\n",
    "\n",
    "# Average word vectors of each comment with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['cocksucker', 'before', 'you', 'piss', 'around', 'on', 'my', 'work'], ['you', 'be', 'gay', 'or', 'white', 'there', 'be', 'two', 'way', 'why', 'you', 'do', 'erase', 'my', 'comment', 'about', 'ww', 'that', 'holocaust', 'be', 'of', 'jew', 'and', 'not', 'if', 'you', 'be', 'than', 'your', 'head', 'and', 'go', 'to', 'the', 'meeting', 'if', 'you', 'doubt', 'word', 'of', 'the', 'bible', 'that', 'homosexuality', 'be', 'a', 'sin', 'make', 'a', 'on', 'your', 'forehead', 'go', 'to', 'the', 'mass', 'with', 'your', 'gay', 'pal', 'first', 'and', 'last', 'warn', 'you', 'fuck', 'gay', 'i', 'wont', 'appreciate', 'if', 'any', 'more', 'nazi', 'would', 'write', 'in', 'my', 'page', 'i', 'dont', 'wish', 'to', 'talk', 'to', 'you', 'anymore', 'of', 'the', 'dark', 'side'], ['stupid', 'peace', 'of', 'shit', 'stop', 'delete', 'my', 'stuff', 'asshole', 'go', 'die', 'and', 'fall', 'in', 'a', 'hole', 'go', 'to', 'hell']]\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_remove_words_outside_vocab(word2vec_cbow_model, comment_lemma)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim Doc2Vec constants\n",
    "D2V_SIZE = 100          # default is 100\n",
    "D2V_WINDOW_SIZE = 5     # default is 5\n",
    "D2V_MIN_COUNT = 5       # default is 5\n",
    "D2V_DM_MEAN = 1\n",
    "D2V_DBOW_WORDS = 0\n",
    "D2V_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and save Doc2Vec model\n",
    "# Params: \n",
    "    # @documents, @vector_size, @window, @min_count, @dm_mean and @dbow_words are Doc2Vec model params\n",
    "    # TaggedDocument    - @documents:   input corpus with pre-processed tokens in 2D list form\n",
    "    # Int               - @size:        dimensionality of word vectors (typically between 100-300)\n",
    "    # Int               - @window: max distance between current and predicted word in a sentence\n",
    "    # Int               - @min_count:   ignores all words with total frequency lower than this\n",
    "    # Binary            - @dm_mean:     sum or mean of word vectors; 0 - sum, 1 - mean\n",
    "    # Binary            - @dbow_words:  training algorithm, 0 - bow, 1 - skip-gram and bow \n",
    "    # Str               - @file_name:   model name\n",
    "# Output: Model file in directory/repo \n",
    "def doc2vec_create_model(documents, size, window, min_count, dm_mean, dbow_words, file_name):\n",
    "    model = Doc2Vec(documents=documents, vector_size=size, window=window, min_count=min_count, dm_mean=dm_mean, dbow_words=dbow_words)\n",
    "    model.save(\"{0}.model\".format(file_name))\n",
    "\n",
    "# Function to load Doc2Vec model\n",
    "# Params: Str - file name\n",
    "# Returns: Model - doc2vec model\n",
    "def doc2vec_load_model(file_name):\n",
    "    return Word2Vec.load(\"{0}.model\".format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up comments for Gensim Doc2Vec Model\n",
    "comments = [TaggedDocument(comment, [i]) for i, comment in enumerate(comment_lemma)]\n",
    "\n",
    "# Create Doc2Vec model\n",
    "doc2vec_create_model(comments, D2V_SIZE, D2V_WINDOW_SIZE, D2V_MIN_COUNT, D2V_DM_MEAN, D2V_DBOW_WORDS, \"doc2vec_dbow_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x1c2e55a3850>"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "doc2vec_load_model(\"doc2vec_dbow_model\")"
   ]
  }
 ]
}