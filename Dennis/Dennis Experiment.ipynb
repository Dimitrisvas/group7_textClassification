{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "\n",
    "Text Enrichment:\n",
    "\n",
    "Involves augmenting your original text data with information that you did not previously have. Text enrichment provides more semantics to your original text, therby improving its predictive power and the depth of analysis you can perform on your data.\n",
    "\n",
    "\n",
    "References:https://medium.com/opla/text-augmentation-for-machine-learning-tasks-how-to-grow-your-text-dataset-for-classification-38a9a207f88d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Technique 1: Word/sentence shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmentation is challenging a Data process\n",
      "challenging augmentation is process a Data\n",
      "process augmentation a challenging is Data\n",
      "Data augmentation challenging is process a\n",
      "challenging Data process augmentation is a\n",
      "Data augmentation process a challenging is\n",
      "Data challenging is augmentation process a\n",
      "Data augmentation process is challenging a\n",
      "augmentation is Data a challenging process\n",
      "challenging augmentation is Data process a\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import random\n",
    "\n",
    "\n",
    "def shuffle(sen, n):\n",
    "    \n",
    "    # stores newly augmented sentences\n",
    "    new_sentences = []\n",
    "    # returns a list of tokenized words\n",
    "    words = word_tokenize(sen)\n",
    "    \n",
    "    for i in range(n):\n",
    "        random.shuffle(words)\n",
    "        new_sentences.append(' '.join(words))\n",
    "    # remove duplicates    \n",
    "    new_sentences = list(set(new_sentences))\n",
    "    \n",
    "    return new_sentences\n",
    "\n",
    "\n",
    "shuffle_sen = shuffle(\"Data augmentation is a challenging process\",10)\n",
    "\n",
    "for s in shuffle_sen:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique 2: Word replacement\n",
    "\n",
    "This technique aims at replacing some words by their synonyms and expressions by their paraphrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from utils import read_datasets\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "\n",
    "\n",
    "def get_synonyms_lexicon(path):\n",
    "    \n",
    "    \"Fetch set of synonyms from specified path directory \"\n",
    "    \n",
    "    synonyms_lexicon = {}\n",
    "    text_entries = [l.strip() for l in open(path).readlines()]\n",
    "    \n",
    "    for e in text_entries:\n",
    "        e = e.split(' ')\n",
    "        k = e[0]\n",
    "        v = e[1:len(e)]\n",
    "        synonyms_lexicon[k] = v\n",
    "        \n",
    "    return synonyms_lexicon\n",
    "\n",
    "\n",
    "def synonym_replacement(sentence, synonyms_lexicon):\n",
    "    \n",
    "    keys = synonyms_lexicon.keys()\n",
    "    words = word_tokenize(sentence)\n",
    "    n_sentence = sentence\n",
    "    for w in words:\n",
    "        if w not in stoplist:\n",
    "            if w in keys:\n",
    "                n_sentence = n_sentence.replace(w, synonyms_lexicon[w][0])  # we replace with the first synonym\n",
    "    return n_sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  \"\n",
      "\n",
      "I think the entire article should be tagged as \"\"politicized\"\", and the part about future trends as \"\"speculative\"\".  My point was that scientists are learners, and we are increasingly using them as know it all teachers.  Yes, they probably can make educated guesses better than anyone else, but its still an educated guess.  Also, if you didn't notice, I am totally against spewing unnecessary carbon as this statement I think this article superbly misses the point that pollution and waste of ALL TYPES are stupid ways to go about doing things where better ways are known, and we are making guinea pigs of all our planet's species points out.  \"\n",
      "new_comment:  \"\n",
      "\n",
      "I think the entire article should be tagged as \"\"politicized\"\", and the part about future trends as \"\"speculative\"\".  My point was that scientists are learners, and we are increasingly using them as know it all teachers.  Yes, they probably can make educated guesses better than anyone else, but its still an educated guess.  Also, if you didn't notice, I am totally against spewing unnecessary carbon as this statement I think this article superbly misses the point that pollution and waste of ALL TYPES are stupid ways to go about doing things where better ways are known, and we are making guinea pigs of all our planet's species points out.  \"\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    text = 'Many customers initiated a return process of the product as it was not suitable for use.' \n",
    "    \n",
    "    x, y = read_datasets()\n",
    "    test = x['comment_text'][622]\n",
    "\n",
    "    \n",
    "    \n",
    "    # Implement batching to optimise the process\n",
    "\n",
    "    synonyms_lexicon = get_synonyms_lexicon('./ppdb-2.0-s-all')\n",
    "    \n",
    "    #for comment in x:\n",
    "    new_comment = synonym_replacement(test, synonyms_lexicon)\n",
    "    print('test: ', test)\n",
    "    print('new_comment: ', new_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2 \n",
    "\n",
    "- Regularisation\n",
    "- l2 (wegiht decay, or ridge regression) adds a penalty for the complexity of a model\n",
    "- l1 makes the less important features to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
